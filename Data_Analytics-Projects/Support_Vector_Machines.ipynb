{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Headers\n",
    "# You are welcome to add additional headers here if you wish\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable inline mode for matplotlib so that Jupyter displays graphs\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heart Dataset \n",
    "\n",
    "In this project we will work with some patients dataset. \n",
    "\n",
    "We have access to 303 patients data. The features are listed below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPain</th>\n",
       "      <th>RestBP</th>\n",
       "      <th>Chol</th>\n",
       "      <th>Fbs</th>\n",
       "      <th>RestECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExAng</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Thal</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>typical</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fixed</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>reversable</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>nonanginal</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>nontypical</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>typical</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>reversable</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>reversable</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>reversable</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>nontypical</td>\n",
       "      <td>130</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>nonanginal</td>\n",
       "      <td>138</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age  Sex     ChestPain  RestBP  Chol  Fbs  RestECG  MaxHR  ExAng  \\\n",
       "0     63    1       typical     145   233    1        2    150      0   \n",
       "1     67    1  asymptomatic     160   286    0        2    108      1   \n",
       "2     67    1  asymptomatic     120   229    0        2    129      1   \n",
       "3     37    1    nonanginal     130   250    0        0    187      0   \n",
       "4     41    0    nontypical     130   204    0        2    172      0   \n",
       "..   ...  ...           ...     ...   ...  ...      ...    ...    ...   \n",
       "298   45    1       typical     110   264    0        0    132      0   \n",
       "299   68    1  asymptomatic     144   193    1        0    141      0   \n",
       "300   57    1  asymptomatic     130   131    0        0    115      1   \n",
       "301   57    0    nontypical     130   236    0        2    174      0   \n",
       "302   38    1    nonanginal     138   175    0        0    173      0   \n",
       "\n",
       "     Oldpeak  Slope   Ca        Thal Target  \n",
       "0        2.3      3  0.0       fixed     No  \n",
       "1        1.5      2  3.0      normal    Yes  \n",
       "2        2.6      2  2.0  reversable    Yes  \n",
       "3        3.5      3  0.0      normal     No  \n",
       "4        1.4      1  0.0      normal     No  \n",
       "..       ...    ...  ...         ...    ...  \n",
       "298      1.2      2  0.0  reversable    Yes  \n",
       "299      3.4      2  2.0  reversable    Yes  \n",
       "300      1.2      2  1.0  reversable    Yes  \n",
       "301      0.0      2  1.0      normal    Yes  \n",
       "302      0.0      1  NaN      normal     No  \n",
       "\n",
       "[303 rows x 14 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "heart_df = pd.read_csv(\"Heart.csv\")\n",
    "heart_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Age:** The person’s age in years\n",
    "\n",
    "**Sex:** The person’s sex (1 = male, 0 = female)\n",
    "\n",
    "**ChestPain:** chest pain type\n",
    "* Value 0: asymptomatic\n",
    "* Value 1: atypical angina\n",
    "* Value 2: non-anginal pain\n",
    "* Value 3: typical angina\n",
    "\n",
    "**RestBP:** The person’s resting blood pressure (mm Hg on admission to the hospital)\n",
    "\n",
    "**Chol:** The person’s cholesterol measurement in mg/dl\n",
    "\n",
    "**Fbs:** The person’s fasting blood sugar (> 120 mg/dl, 1 = true; 0 = false)\n",
    "\n",
    "**RestECG**: resting electrocardiographic results\n",
    "* Value 0: showing probable or definite left ventricular hypertrophy by Estes’ criteria\n",
    "* Value 1: normal\n",
    "* Value 2: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    "\n",
    "**MaxHR:** The person’s maximum heart rate achieved\n",
    "\n",
    "**ExAng:** Exercise induced angina (1 = yes; 0 = no)\n",
    "\n",
    "**Oldpeak:** ST depression induced by exercise relative to rest (‘ST’ relates to positions on the ECG plot.)\n",
    "\n",
    "**Slope:** the slope of the peak exercise ST segment\n",
    "* 0: downsloping; \n",
    "* 1: flat; \n",
    "* 2: upsloping\n",
    "\n",
    "**Ca:** The number of major vessels (0–3)\n",
    "\n",
    "**Thal:** A blood disorder called thalassemia \n",
    "* Value 0: NULL (dropped from the dataset previously\n",
    "* Value 1: fixed defect (no blood flow in some part of the heart)\n",
    "* Value 2: normal blood flow\n",
    "* Value 3: reversible defect (a blood flow is observed but it is not normal)\n",
    "\n",
    "**Target:** Heart disease (1 = yes; 0 = no)\n",
    "- **Note**: For Q1, Q4, and Q5. your labels can be arbitrary. But, for Q3, you will need to ensure that \"Yes\" = 1 and \"No\" = -1, as an SVM predicts 1 and -1, not 1 and 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 \n",
    "\n",
    "**Implement SVM using libraries**: We want to use a **support vector machine** to predict if each patient will have heart problems or not. The column \"Target\" in our datasets includes data about heart diseases. If the patient had heart disease, it is labeled \"Yes\", otherwise it is labeled \"No\".\n",
    "\n",
    "Please prepare your dataset for predicting heart disease (\"Target\" column) by using 3 features:\n",
    "\n",
    "- Age of the patient (Column **\"Age\"**)\n",
    "- Gender of the patient (male or female - Column **\"Sex\"**)\n",
    "- Cholestrol level of the patient (Column **\"Chol\"**) \n",
    "\n",
    "\n",
    "Split your data into 80% traning data and 20% test data.\n",
    "\n",
    "Finally, implement a Support Vector Machine using Scikit-Learn and train it on your training set.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Put the features into an \"X\" array, and target variable into a \"y\" array.\n",
    "X = heart_df[[\"Age\", \"Sex\", \"Chol\"]].to_numpy()\n",
    "y = heart_df[\"Target\"].to_numpy()\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 57   0 303]\n",
      " [ 58   0 319]\n",
      " [ 54   1 206]\n",
      " [ 56   1 221]\n",
      " [ 48   1 245]\n",
      " [ 65   1 282]\n",
      " [ 50   0 254]\n",
      " [ 45   0 236]\n",
      " [ 60   1 206]\n",
      " [ 64   1 212]\n",
      " [ 57   1 274]\n",
      " [ 54   1 309]\n",
      " [ 41   0 268]\n",
      " [ 46   0 177]\n",
      " [ 53   1 246]\n",
      " [ 43   1 177]\n",
      " [ 45   1 308]\n",
      " [ 58   0 283]\n",
      " [ 66   1 246]\n",
      " [ 77   1 304]\n",
      " [ 62   0 263]\n",
      " [ 56   1 236]\n",
      " [ 71   0 149]\n",
      " [ 39   1 321]\n",
      " [ 57   1 335]\n",
      " [ 60   0 240]\n",
      " [ 57   0 354]\n",
      " [ 54   0 214]\n",
      " [ 44   1 226]\n",
      " [ 55   0 327]\n",
      " [ 70   1 269]\n",
      " [ 29   1 204]\n",
      " [ 70   1 174]\n",
      " [ 58   0 225]\n",
      " [ 52   1 223]\n",
      " [ 42   0 265]\n",
      " [ 50   1 196]\n",
      " [ 54   1 283]\n",
      " [ 55   0 342]\n",
      " [ 52   1 212]\n",
      " [ 57   1 276]\n",
      " [ 44   1 233]\n",
      " [ 54   1 266]\n",
      " [ 62   0 164]\n",
      " [ 58   1 224]\n",
      " [ 62   0 394]\n",
      " [ 42   1 295]\n",
      " [ 62   1 281]\n",
      " [ 45   0 234]\n",
      " [ 57   1 192]\n",
      " [ 46   1 311]\n",
      " [ 43   1 247]\n",
      " [ 58   1 270]\n",
      " [ 46   1 197]\n",
      " [ 46   1 249]\n",
      " [ 59   1 234]\n",
      " [ 41   1 250]\n",
      " [ 60   1 185]\n",
      " [ 58   1 220]\n",
      " [ 42   1 315]\n",
      " [ 44   1 220]\n",
      " [ 52   1 186]\n",
      " [ 46   1 231]\n",
      " [ 41   0 204]\n",
      " [ 45   1 260]\n",
      " [ 54   1 239]\n",
      " [ 54   0 267]\n",
      " [ 49   0 271]\n",
      " [ 44   1 235]\n",
      " [ 58   1 259]\n",
      " [ 67   0 564]\n",
      " [ 37   0 215]\n",
      " [ 41   1 157]\n",
      " [ 64   1 263]\n",
      " [ 51   1 222]\n",
      " [ 58   1 240]\n",
      " [ 51   1 227]\n",
      " [ 35   1 198]\n",
      " [ 62   1 231]\n",
      " [ 47   1 204]\n",
      " [ 58   1 216]\n",
      " [ 56   1 240]\n",
      " [ 58   1 234]\n",
      " [ 59   1 218]\n",
      " [ 57   0 236]\n",
      " [ 54   1 258]\n",
      " [ 43   1 303]\n",
      " [ 41   0 198]\n",
      " [ 60   0 258]\n",
      " [ 39   0 220]\n",
      " [ 57   1 131]\n",
      " [ 63   0 197]\n",
      " [ 49   0 269]\n",
      " [ 70   1 245]\n",
      " [ 44   1 263]\n",
      " [ 43   1 247]\n",
      " [ 57   0 241]\n",
      " [ 68   0 211]\n",
      " [ 66   1 228]\n",
      " [ 45   0 160]\n",
      " [ 67   1 299]\n",
      " [ 56   1 193]\n",
      " [ 43   0 213]\n",
      " [ 63   1 254]\n",
      " [ 51   0 308]\n",
      " [ 54   1 273]\n",
      " [ 56   0 409]\n",
      " [ 64   0 303]\n",
      " [ 65   1 254]\n",
      " [ 48   1 255]\n",
      " [ 58   0 197]\n",
      " [ 54   0 304]\n",
      " [ 61   1 260]\n",
      " [ 51   1 299]\n",
      " [ 59   1 288]\n",
      " [ 53   0 234]\n",
      " [ 63   0 195]\n",
      " [ 61   1 234]\n",
      " [ 61   0 330]\n",
      " [ 41   1 235]\n",
      " [ 50   1 200]\n",
      " [ 60   1 282]\n",
      " [ 51   0 305]\n",
      " [ 44   0 141]\n",
      " [ 66   0 278]\n",
      " [ 61   1 166]\n",
      " [ 55   0 205]\n",
      " [ 76   0 197]\n",
      " [ 58   1 300]\n",
      " [ 58   0 248]\n",
      " [ 63   1 233]\n",
      " [ 56   1 283]\n",
      " [ 58   1 230]\n",
      " [ 55   1 289]\n",
      " [ 65   0 225]\n",
      " [ 51   1 213]\n",
      " [ 57   1 168]\n",
      " [ 51   1 175]\n",
      " [ 54   1 286]\n",
      " [ 39   1 219]\n",
      " [ 49   1 188]\n",
      " [ 51   1 261]\n",
      " [ 48   1 229]\n",
      " [ 57   1 232]\n",
      " [ 62   0 294]\n",
      " [ 44   0 242]\n",
      " [ 58   1 211]\n",
      " [ 64   1 227]\n",
      " [ 44   1 197]\n",
      " [ 61   0 307]\n",
      " [ 69   1 254]\n",
      " [ 47   1 243]\n",
      " [ 55   1 353]\n",
      " [ 66   0 226]\n",
      " [ 42   1 244]\n",
      " [ 59   1 221]\n",
      " [ 50   0 244]\n",
      " [ 63   0 407]\n",
      " [ 38   1 231]\n",
      " [ 52   1 199]\n",
      " [ 53   0 216]\n",
      " [ 53   1 203]\n",
      " [ 62   0 268]\n",
      " [ 39   0 199]\n",
      " [ 67   0 277]\n",
      " [ 49   1 149]\n",
      " [ 54   0 201]\n",
      " [ 38   1 175]\n",
      " [ 71   0 265]\n",
      " [ 46   0 243]\n",
      " [ 60   0 318]\n",
      " [ 51   1 245]\n",
      " [ 54   1 232]\n",
      " [ 37   1 250]\n",
      " [ 47   1 257]\n",
      " [ 56   1 184]\n",
      " [ 59   1 271]\n",
      " [ 62   1 208]\n",
      " [ 65   0 417]\n",
      " [ 74   0 269]\n",
      " [ 35   0 183]\n",
      " [ 59   0 249]\n",
      " [ 65   1 177]\n",
      " [ 67   1 286]\n",
      " [ 62   0 244]\n",
      " [ 48   0 275]\n",
      " [ 56   1 256]\n",
      " [ 53   1 197]\n",
      " [ 50   1 233]\n",
      " [ 41   1 214]\n",
      " [ 43   1 315]\n",
      " [ 50   0 219]\n",
      " [ 59   1 270]\n",
      " [ 34   1 182]\n",
      " [ 42   1 240]\n",
      " [ 59   1 212]\n",
      " [ 62   0 209]\n",
      " [ 60   0 178]\n",
      " [ 42   0 209]\n",
      " [ 35   1 192]\n",
      " [ 64   1 246]\n",
      " [ 65   0 269]\n",
      " [ 59   1 204]\n",
      " [ 60   1 293]\n",
      " [ 58   1 318]\n",
      " [ 57   1 289]\n",
      " [ 45   1 309]\n",
      " [ 54   1 188]\n",
      " [ 45   1 264]\n",
      " [ 35   1 282]\n",
      " [ 60   1 258]\n",
      " [ 63   1 330]\n",
      " [ 66   0 228]\n",
      " [ 48   1 274]\n",
      " [ 48   1 222]\n",
      " [ 55   0 250]\n",
      " [ 52   1 233]\n",
      " [ 67   1 212]\n",
      " [ 52   1 325]\n",
      " [ 61   1 243]\n",
      " [ 67   1 229]\n",
      " [ 59   1 326]\n",
      " [ 55   1 262]\n",
      " [ 63   1 187]\n",
      " [ 51   0 295]\n",
      " [ 56   1 249]\n",
      " [ 63   0 269]\n",
      " [ 64   1 335]\n",
      " [ 65   1 248]\n",
      " [ 50   1 243]\n",
      " [ 56   0 288]\n",
      " [ 43   0 341]\n",
      " [ 59   1 239]\n",
      " [ 34   0 210]\n",
      " [ 52   1 230]\n",
      " [ 41   1 172]\n",
      " [ 55   1 217]\n",
      " [ 59   1 177]\n",
      " [ 68   1 274]\n",
      " [ 54   1 239]\n",
      " [ 52   0 196]\n",
      " [ 52   1 201]]\n"
     ]
    }
   ],
   "source": [
    "# Split your \"X\" and \"y\" arrays into training and testing sets. \n",
    "# You may use scikit-learn to do this.\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X_train = ...\n",
    "#X_test = ...\n",
    "#y_train = ...\n",
    "#y_test = ...\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an SVM using SciKit-Learn, and train it on your training set.\n",
    "from sklearn import svm\n",
    "svm_model = svm.SVC()\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Q2\n",
    "\n",
    "Calculate the accuracy, Precision, recall and F1 score of your **SVM** implementaion from Task 1 on the testing dataset. \n",
    "Print the results. You may use library methods for this task if you choose to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.52      0.67      0.59        33\n",
      "         Yes       0.42      0.29      0.34        28\n",
      "\n",
      "    accuracy                           0.49        61\n",
      "   macro avg       0.47      0.48      0.46        61\n",
      "weighted avg       0.48      0.49      0.47        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = svm_model.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Q3 \n",
    "\n",
    "**Implement SVM without using libraries**: Implement a SVM from scratch using the hinge loss \n",
    "function and gradient descent. Try to replicate the same results as you got from the scikit-learn\n",
    "SVM. Report the accuracy, precision, recall, and F1-score of your model.\n",
    "\n",
    "- You can re-use your training/testing set from Q1, but you may need to rename your target variable (e.g. \"No\", \"Yes\") to a number. If so,\n",
    "  assign 1 to \"Yes\"es and -1 to \"No\"s. (**Note: setting \"No\"s to -1 is very important here!**)\n",
    "- Do as many iterations as needed, with a maximum of **100 iterations**.\n",
    "- Use a very small learning rate for checking your GD implementation. \n",
    "- You are allowed to use your choice of learning rate, like using 0.0001, 0.001 or 0.01 or 0.1 or higher. \n",
    "- Visualize your costs with a plot. \n",
    "- No need to add an y-intercept in this task.\n",
    "- You can use libraries to report the accuracy, precision, recall and F1-score. \n",
    "\n",
    "\n",
    "**Hint**: Here are the formulae for hinge loss and its gradient:\n",
    "\n",
    "Hinge loss function (with regularization):\n",
    "$$ \n",
    "cost = \\frac{ \\lambda }{ 2 } ||w||^2 + \\frac{1}{N} \\sum_i^n max(0, 1 - y_i (w \\cdot x_i))\n",
    "$$\n",
    "- $x_i$: Training sample $x_i$\n",
    "- $y_i$: Training label $y_i$\n",
    "- $w$: SVM weights\n",
    "- $N$: Number of elements\n",
    "- $\\lambda$: Regularization parameter, $= \\frac{1}{N \\times c}$\n",
    "\n",
    "Gradient of hinge loss:\n",
    "\n",
    "$$gradient(w)=\\frac{1}{N}\\sum_i^n\n",
    "\\begin{cases}\n",
    "w &  \\text{ if } max(0, y_i (w \\cdot x_i)) = 0 \\\\\n",
    "w - c y_i x_i &  \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implement hinge cost and its gradient function.\n",
    "def hinge_loss(X, y, W, c):\n",
    "    \"\"\"Calculate the hinge loss function, with regularization.\n",
    "    \n",
    "    Parameters:\n",
    "        X: Training set samples\n",
    "        y: Training set labels\n",
    "        W: Current weights of the SVM.\n",
    "        c: The term c in the formula above (used to define lambda)\n",
    "\n",
    "    Returns:\n",
    "        hinge_loss: The hinge loss.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    distances = 1 - y * (np.dot(X, W))\n",
    "    \n",
    "    # This is our max(0, distance). \n",
    "    distances[distances < 0] = 0 \n",
    "    \n",
    "    hinge_loss = c * (np.sum(distances) / n)\n",
    "    return hinge_loss\n",
    "\n",
    "def hinge_loss_gradient(X, y, W, c):\n",
    "    \"\"\"Calculate the gradient of the hinge loss function.\n",
    "    \n",
    "    Parameters:\n",
    "        X: Training set samples\n",
    "        y: Training set labels\n",
    "        W: Current weights of the SVM.\n",
    "        c: The term c in the formula above (used to define lambda)\n",
    "\n",
    "    Returns:\n",
    "        dW: The gradient of the hinge loss with respect to each feature.\n",
    "    \"\"\"\n",
    "    if type(y) == np.float64:\n",
    "        y = np.array([y])\n",
    "        X = np.array([X])\n",
    "        \n",
    "    distance = 1 - (y * np.dot(X, W))\n",
    "    \n",
    "    dw = np.zeros(len(W))\n",
    "    \n",
    "    for ind, d in enumerate(distance):\n",
    "        \n",
    "        if (d < 0):\n",
    "            di = W\n",
    "        else:\n",
    "            di = W - (c * y[ind] * X[ind])\n",
    "            \n",
    "            \n",
    "        dw += di\n",
    "    \n",
    "    dw = dw/len(y)  # average\n",
    "    return dw\n",
    "\n",
    "def predict_svm(X, W):\n",
    "    \"\"\"Predict the label for a set of samples, given an SVM\n",
    "    with weights W. \n",
    "    \n",
    "    (Hint: You won't need this for your gradient descent, just your\n",
    "     final metrics collection)\n",
    "     \n",
    "    Parameters:\n",
    "        X: Training set samples\n",
    "        W: Current weights of the SVM.\n",
    "        \n",
    "    Returns:\n",
    "        y_pred: Predicted classification of the samples.\n",
    "    \"\"\"\n",
    "    y_pred = np.where(np.dot(X,W) < 0,-1,1)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 Cost is: 0.01 weights [0. 0. 0.]\n",
      "Iteration 1 Cost is: 0.00997893968308176 weights [-2.18181818e-05  5.78512397e-07 -1.43471074e-04]\n",
      "Iteration 2 Cost is: 0.009957900426480433 weights [-4.36145455e-05  1.15644628e-06 -2.86798678e-04]\n",
      "Iteration 3 Cost is: 0.009936882209135706 weights [-6.53891127e-05  1.73380223e-06 -4.29982953e-04]\n",
      "Iteration 4 Cost is: 0.009915885010008325 weights [-8.71419054e-05  2.31058083e-06 -5.73024045e-04]\n",
      "Iteration 5 Cost is: 0.009894908808080072 weights [-1.08872945e-04  2.88678264e-06 -7.15922095e-04]\n",
      "Iteration 6 Cost is: 0.009873953582353755 weights [-1.30582254e-04  3.46240826e-06 -8.58677247e-04]\n",
      "Iteration 7 Cost is: 0.009853019311853157 weights [-1.52269854e-04  4.03745824e-06 -1.00128964e-03]\n",
      "Iteration 8 Cost is: 0.00983210597562306 weights [-1.73935766e-04  4.61193318e-06 -1.14375943e-03]\n",
      "Iteration 9 Cost is: 0.009811213552729193 weights [-1.95580012e-04  5.18583365e-06 -1.28608674e-03]\n",
      "Iteration 10 Cost is: 0.00979034202225822 weights [-2.17202614e-04  5.75916021e-06 -1.42827173e-03]\n",
      "Iteration 11 Cost is: 0.009769491363317716 weights [-2.38803593e-04  6.33191345e-06 -1.57031453e-03]\n",
      "Iteration 12 Cost is: 0.009748661555036155 weights [-2.60382971e-04  6.90409393e-06 -1.71221529e-03]\n",
      "Iteration 13 Cost is: 0.009730519165576908 weights [-2.81940770e-04  7.47570223e-06 -1.85397415e-03]\n",
      "Iteration 14 Cost is: 0.009715944405922286 weights [-3.00708416e-04  8.04673893e-06 -1.97228547e-03]\n",
      "Iteration 15 Cost is: 0.009701384221027332 weights [-3.19457294e-04  8.61720458e-06 -2.09047847e-03]\n",
      "Iteration 16 Cost is: 0.009686838596317264 weights [-3.38187424e-04  9.18709978e-06 -2.20855328e-03]\n",
      "Iteration 17 Cost is: 0.009672307517231903 weights [-3.56898823e-04  9.75642507e-06 -2.32651002e-03]\n",
      "Iteration 18 Cost is: 0.009659597039355042 weights [-3.75591511e-04  1.03251810e-05 -2.44434880e-03]\n",
      "Iteration 19 Cost is: 0.009650104708061349 weights [-3.91579556e-04  1.08933683e-05 -2.54483833e-03]\n",
      "Iteration 20 Cost is: 0.009642631092322627 weights [-4.04989629e-04  1.14609873e-05 -2.62894639e-03]\n",
      "Iteration 21 Cost is: 0.009635164950199632 weights [-4.18386292e-04  1.20280387e-05 -2.71297033e-03]\n",
      "Iteration 22 Cost is: 0.00962831441866767 weights [-4.31769559e-04  1.25945231e-05 -2.79691026e-03]\n",
      "Iteration 23 Cost is: 0.00962339083291607 weights [-4.42784071e-04  1.31604409e-05 -2.86613814e-03]\n",
      "Iteration 24 Cost is: 0.00962012479301547 weights [-4.51514840e-04  1.37257929e-05 -2.92116456e-03]\n",
      "Iteration 25 Cost is: 0.009616862019154763 weights [-4.60236879e-04  1.42905795e-05 -2.97613596e-03]\n",
      "Iteration 26 Cost is: 0.009613993523817033 weights [-4.68950196e-04  1.48548013e-05 -3.03105239e-03]\n",
      "Iteration 27 Cost is: 0.0096125337249425 weights [-4.75506039e-04  1.54597812e-05 -3.07248414e-03]\n",
      "Iteration 28 Cost is: 0.009612255481908083 weights [-4.77964417e-04  1.61054785e-05 -3.08746951e-03]\n",
      "Iteration 29 Cost is: 0.009611977517116706 weights [-4.80420337e-04  1.67505300e-05 -3.10243989e-03]\n",
      "Iteration 30 Cost is: 0.009611812462499383 weights [-4.82873801e-04  1.73949365e-05 -3.11739530e-03]\n",
      "Iteration 31 Cost is: 0.009611801502096165 weights [-4.83547952e-04  1.80800209e-05 -3.11931923e-03]\n",
      "Iteration 32 Cost is: 0.009611790552653352 weights [-4.84221429e-04  1.87644202e-05 -3.12124123e-03]\n",
      "Iteration 33 Cost is: 0.009611779614159979 weights [-4.84894232e-04  1.94481351e-05 -3.12316132e-03]\n",
      "Iteration 34 Cost is: 0.009611768686605097 weights [-4.85566363e-04  2.01311663e-05 -3.12507948e-03]\n",
      "Iteration 35 Cost is: 0.009611757769977788 weights [-4.86237821e-04  2.08135145e-05 -3.12699572e-03]\n",
      "Iteration 36 Cost is: 0.009611746864267095 weights [-4.86908608e-04  2.14951803e-05 -3.12891005e-03]\n",
      "Iteration 37 Cost is: 0.00961173596946212 weights [-4.87578725e-04  2.21761645e-05 -3.13082246e-03]\n",
      "Iteration 38 Cost is: 0.009611725085551939 weights [-4.88248171e-04  2.28564677e-05 -3.13273296e-03]\n",
      "Iteration 39 Cost is: 0.00961171421252567 weights [-4.88916947e-04  2.35360905e-05 -3.13464155e-03]\n",
      "Iteration 40 Cost is: 0.009611703350372437 weights [-4.89585055e-04  2.42150338e-05 -3.13654823e-03]\n",
      "Iteration 41 Cost is: 0.009611692499081342 weights [-4.90252495e-04  2.48932981e-05 -3.13845300e-03]\n",
      "Iteration 42 Cost is: 0.009611681658641555 weights [-4.90919267e-04  2.55708841e-05 -3.14035587e-03]\n",
      "Iteration 43 Cost is: 0.00961167082904219 weights [-4.91585373e-04  2.62477926e-05 -3.14225684e-03]\n",
      "Iteration 44 Cost is: 0.009611660010272437 weights [-4.92250812e-04  2.69240241e-05 -3.14415590e-03]\n",
      "Iteration 45 Cost is: 0.009611649202321451 weights [-4.92915586e-04  2.75995795e-05 -3.14605307e-03]\n",
      "Iteration 46 Cost is: 0.009611638405178414 weights [-4.93579695e-04  2.82744592e-05 -3.14794834e-03]\n",
      "Iteration 47 Cost is: 0.009611627618832523 weights [-4.94243140e-04  2.89486641e-05 -3.14984171e-03]\n",
      "Iteration 48 Cost is: 0.00961164084552656 weights [-4.94905922e-04  2.96221948e-05 -3.15173319e-03]\n",
      "Iteration 49 Cost is: 0.009611673001854873 weights [-4.93336636e-04  3.03363742e-05 -3.14085419e-03]\n",
      "Iteration 50 Cost is: 0.009611662180912306 weights [-4.94000324e-04  3.10085172e-05 -3.14275466e-03]\n",
      "Iteration 51 Cost is: 0.009611651370790677 weights [-4.94663349e-04  3.16799880e-05 -3.14465322e-03]\n",
      "Iteration 52 Cost is: 0.009611640571479172 weights [-4.95325710e-04  3.23507874e-05 -3.14654989e-03]\n",
      "Iteration 53 Cost is: 0.009611629782966992 weights [-4.95987409e-04  3.30209159e-05 -3.14844467e-03]\n",
      "Iteration 54 Cost is: 0.009611628907165845 weights [-4.96648446e-04  3.36903743e-05 -3.15033754e-03]\n",
      "Iteration 55 Cost is: 0.009611675161663231 weights [-4.95077418e-04  3.44004856e-05 -3.13945993e-03]\n",
      "Iteration 56 Cost is: 0.009611664338560855 weights [-4.95739365e-04  3.50685645e-05 -3.14136180e-03]\n",
      "Iteration 57 Cost is: 0.00961165352628158 weights [-4.96400651e-04  3.57359752e-05 -3.14326176e-03]\n",
      "Iteration 58 Cost is: 0.009611642724814587 weights [-4.97061275e-04  3.64027186e-05 -3.14515982e-03]\n",
      "Iteration 59 Cost is: 0.009611631934149057 weights [-4.97721238e-04  3.70687952e-05 -3.14705598e-03]\n",
      "Iteration 60 Cost is: 0.009611621154274204 weights [-4.98380542e-04  3.77342058e-05 -3.14895025e-03]\n",
      "Iteration 61 Cost is: 0.009611631876327336 weights [-4.99039186e-04  3.83989509e-05 -3.15084262e-03]\n",
      "Iteration 62 Cost is: 0.0096116665502192 weights [-4.97465767e-04  3.91043536e-05 -3.13996450e-03]\n",
      "Iteration 63 Cost is: 0.009611655735728261 weights [-4.98125326e-04  3.97677286e-05 -3.14186586e-03]\n",
      "Iteration 64 Cost is: 0.009611644932051827 weights [-4.98784225e-04  4.04304402e-05 -3.14376532e-03]\n",
      "Iteration 65 Cost is: 0.009611634139179061 weights [-4.99442466e-04  4.10924891e-05 -3.14566287e-03]\n",
      "Iteration 66 Cost is: 0.009611623357099164 weights [-5.00100048e-04  4.17538759e-05 -3.14755853e-03]\n",
      "Iteration 67 Cost is: 0.009611619991647452 weights [-5.00756973e-04  4.24146014e-05 -3.14945230e-03]\n",
      "Iteration 68 Cost is: 0.009611668748640726 weights [-4.99181836e-04  4.31159885e-05 -3.13857557e-03]\n",
      "Iteration 69 Cost is: 0.009611657931951362 weights [-4.99839679e-04  4.37753518e-05 -3.14047832e-03]\n",
      "Iteration 70 Cost is: 0.00961164712607871 weights [-5.00496864e-04  4.44340558e-05 -3.14237916e-03]\n",
      "Iteration 71 Cost is: 0.009611636331011916 weights [-5.01153392e-04  4.50921011e-05 -3.14427811e-03]\n",
      "Iteration 72 Cost is: 0.009611625546740186 weights [-5.01809263e-04  4.57494883e-05 -3.14617515e-03]\n",
      "Iteration 73 Cost is: 0.009611614773252725 weights [-5.02464479e-04  4.64062182e-05 -3.14807030e-03]\n",
      "Iteration 74 Cost is: 0.009611623023030652 weights [-5.03119039e-04  4.70622913e-05 -3.14996355e-03]\n",
      "Iteration 75 Cost is: 0.0096116601819534 weights [-5.01541540e-04  4.77590307e-05 -3.13908631e-03]\n",
      "Iteration 76 Cost is: 0.009611649373830725 weights [-5.02197023e-04  4.84137510e-05 -3.14098855e-03]\n",
      "Iteration 77 Cost is: 0.009611638576516184 weights [-5.02851851e-04  4.90678166e-05 -3.14288888e-03]\n",
      "Iteration 78 Cost is: 0.00961162778999896 weights [-5.03506024e-04  4.97212281e-05 -3.14478732e-03]\n",
      "Iteration 79 Cost is: 0.009611617014268242 weights [-5.04159543e-04  5.03739862e-05 -3.14668385e-03]\n",
      "Iteration 80 Cost is: 0.009611611191337927 weights [-5.04812408e-04  5.10260915e-05 -3.14857849e-03]\n",
      "Iteration 81 Cost is: 0.009611662418489115 weights [-5.03233215e-04  5.17188671e-05 -3.13770264e-03]\n",
      "Iteration 82 Cost is: 0.009611651608129925 weights [-5.03887007e-04  5.23696276e-05 -3.13960626e-03]\n",
      "Iteration 83 Cost is: 0.00961164080858108 weights [-5.04540145e-04  5.30197373e-05 -3.14150797e-03]\n",
      "Iteration 84 Cost is: 0.009611630019831782 weights [-5.05192629e-04  5.36691969e-05 -3.14340779e-03]\n",
      "Iteration 85 Cost is: 0.009611619241871228 weights [-5.05844461e-04  5.43180070e-05 -3.14530570e-03]\n",
      "Iteration 86 Cost is: 0.009611608474688652 weights [-5.06495642e-04  5.49661684e-05 -3.14720172e-03]\n",
      "Iteration 87 Cost is: 0.009611614284138788 weights [-5.07146171e-04  5.56136815e-05 -3.14909584e-03]\n",
      "Iteration 88 Cost is: 0.009611653895980145 weights [-5.05564645e-04  5.63018695e-05 -3.13821947e-03]\n",
      "Iteration 89 Cost is: 0.009611643094143458 weights [-5.06216105e-04  5.69480470e-05 -3.14012257e-03]\n",
      "Iteration 90 Cost is: 0.0096116323031086 weights [-5.06866913e-04  5.75935783e-05 -3.14202377e-03]\n",
      "Iteration 91 Cost is: 0.00961162152286477 weights [-5.07517071e-04  5.82384640e-05 -3.14392307e-03]\n",
      "Iteration 92 Cost is: 0.009611610753401193 weights [-5.08166579e-04  5.88827049e-05 -3.14582047e-03]\n",
      "Iteration 93 Cost is: 0.009611602504748525 weights [-5.08815437e-04  5.95263015e-05 -3.14771597e-03]\n",
      "Iteration 94 Cost is: 0.00961165617013755 weights [-5.07232242e-04  6.02105769e-05 -3.13684098e-03]\n",
      "Iteration 95 Cost is: 0.00961164536602669 weights [-5.07882034e-04  6.08528456e-05 -3.13874547e-03]\n",
      "Iteration 96 Cost is: 0.009611634572719947 weights [-5.08531177e-04  6.14944721e-05 -3.14064804e-03]\n",
      "Iteration 97 Cost is: 0.009611623790206516 weights [-5.09179671e-04  6.21354570e-05 -3.14254872e-03]\n",
      "Iteration 98 Cost is: 0.009611613018475596 weights [-5.09827516e-04  6.27758009e-05 -3.14444749e-03]\n",
      "Iteration 99 Cost is: 0.009611602257516415 weights [-5.10474713e-04  6.34155044e-05 -3.14634437e-03]\n"
     ]
    }
   ],
   "source": [
    "# Implement an iterative SVM trained via gradient descent. \n",
    "# Make sure your y-labels are in the right format (-1 for \"No\", 1 for \"Yes\")\n",
    "y_train[y_train == \"Yes\"] = 1\n",
    "y_train[y_train == \"No\"] = -1\n",
    "\n",
    "weights = np.zeros(3)\n",
    "num_iterations = 100\n",
    "lr = 0.001\n",
    "c = 0.01\n",
    "cost_list = []\n",
    "\n",
    "# Implement gradient descent here.\n",
    "for i in range(0,num_iterations):\n",
    "    cost = hinge_loss(X_train,y_train,weights,c)\n",
    "    print(\"Iteration\", i , \"Cost is:\", cost, \"weights\", weights)\n",
    "    cost_list.append(cost)\n",
    "    grad = hinge_loss_gradient(X_train, y_train, weights, c)\n",
    "    weights = weights - lr * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkcAAAGdCAYAAAAYDtcjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABR4ElEQVR4nO3dfVhU95k//vcwzAMgEBHlQc0IJgHRBmWICpFosiyosWrtt2pNqelWr7DVGLRJfEp+3a3ZSrZNm7rxIcmSXg2myhpCdE1VsA2sRkRFtCpWjWJABCkGBlCEmeH+/WGYMM6ADALDwPt1XXNFzrnnnPt8QjJvz/nMOQoRERARERERAMDN2Q0QERER9SUMR0RERERtMBwRERERtcFwRERERNQGwxERERFRGwxHRERERG0wHBERERG1wXBERERE1Ia7sxtwNS0tLbh+/Tq8vb2hUCic3Q4RERF1goigvr4ewcHBcHPr+NwQw5GDrl+/jpEjRzq7DSIiIuqCsrIyjBgxosMahiMHeXt7A7g7uD4+Pk7uhoiIiDqjrq4OI0eOtHyOd4ThyEGtl9J8fHwYjoiIiFxMZ6bEcEI2ERERURsMR0RERERtMBwRERERtcFwRERERNQGwxERERFRGwxHRERERG0wHBERERG1wXBERERE1AbDEREREVEbXQpHW7ZsQUhICLRaLfR6PQ4dOtRhfV5eHvR6PbRaLUJDQ7Ft2zar9efOncP3v/99jBo1CgqFAm+//XaX9isi+Ld/+zcEBwfDw8MD06ZNw7lz56xqmpqa8OKLL8Lf3x9eXl6YPXs2rl275vggEBERUb/kcDjKyMhASkoK1q9fj6KiIsTFxWHGjBkoLS21W19SUoKZM2ciLi4ORUVFWLduHVasWIHMzExLze3btxEaGorU1FQEBgZ2eb//+Z//id/+9rd45513cPz4cQQGBuKf//mfUV9fb6lJSUlBVlYWdu7cicOHD6OhoQGzZs2C2Wx2dCiIiIioPxIHTZw4UZKTk62WhYeHy5o1a+zWv/rqqxIeHm617IUXXpDJkyfbrdfpdPK73/3O4f22tLRIYGCgpKamWtbfuXNHfH19Zdu2bSIiUltbKyqVSnbu3GmpKS8vFzc3N9m/f387R2zNYDAIADEYDJ2qJyIiIudz5PPboTNHzc3NKCwsREJCgtXyhIQEHDlyxO578vPzbeoTExNx4sQJGI3GbttvSUkJKisrrWo0Gg2mTp1qqSksLITRaLSqCQ4Oxrhx49rtv6mpCXV1dVavnnC9thG/y7mIjX8+3yPbJyIios5xKBxVV1fDbDYjICDAanlAQAAqKyvtvqeystJuvclkQnV1dbftt/Wf96tRq9UYPHhwp/vfuHEjfH19La+RI0d2qmdH1d0x4vd/uYQP87+C0dzSI/sgIiKi++vShGyFQmH1s4jYLLtfvb3l3bFfR3u7X83atWthMBgsr7KyMod67qzHhnljsKcKjUYz/nbN0CP7ICIiovtzKBz5+/tDqVTanGWpqqqyOWPTKjAw0G69u7s7hgwZ0m37bZ3Ifb+a5uZm1NTUdLp/jUYDHx8fq1dPcHNTYFLI3fE4euVmj+yDiIiI7s+hcKRWq6HX65GTk2O1PCcnB7GxsXbfExMTY1OfnZ2N6OhoqFSqbttvSEgIAgMDrWqam5uRl5dnqdHr9VCpVFY1FRUVOHv2bLv996bJoX4AGI6IiIicyd3RN6xatQpJSUmIjo5GTEwM3nvvPZSWliI5ORnA3ctQ5eXl+PDDDwEAycnJeOedd7Bq1SosXboU+fn5SEtLw44dOyzbbG5uRnFxseXP5eXlOHXqFAYNGoRHHnmkU/tVKBRISUnBr371Kzz66KN49NFH8atf/Qqenp5YtGgRAMDX1xc//elP8fOf/xxDhgyBn58fXn75ZXznO99BfHz8Awxj95g8+u6ZoxNXa2A0t0Cl5D06iYiIel1Xvg63efNm0el0olarJSoqSvLy8izrFi9eLFOnTrWqz83NlQkTJoharZZRo0bJ1q1brdaXlJQIAJvXvdvpaL8id7/O/4tf/EICAwNFo9HIU089JWfOnLGqaWxslOXLl4ufn594eHjIrFmzpLS0tNPH3pNf5TebW2T8vx8Q3eq9cuLq192+fSIiooHKkc9vhcg3s6OpU+rq6uDr6wuDwdAj84+S0wux/1wlXp0ehp9Ne6Tbt09ERDQQOfL5zes2fcy3846+dnInREREAxPDUR/z7byjr3m/IyIiIidgOOpjWu93dLvZjDPlvN8RERFRb2M46mN4vyMiIiLnYjjqgzjviIiIyHkYjvogzjsiIiJyHoajPojzjoiIiJyH4agP4rwjIiIi52E46qM474iIiMg5GI76KM47IiIicg6Goz6K846IiIicg+Goj2o77yj/MucdERER9RaGoz6sdd5RQQnnHREREfUWhqM+jPOOiIiIeh/DUR/GeUdERES9j+GoD+P9joiIiHofw1Efx/sdERER9S6Goz6O846IiIh6F8NRH9d23tHfrnHeERERUU9jOOrjOO+IiIiodzEcuYBv5x0xHBEREfU0hiMX8O28oxrOOyIiIuphDEcuoHXeUaOR846IiIh6GsORC+C8IyIiot7DcOQiOO+IiIiodzAcuQjOOyIiIuodDEcu4rFh3vDzUnPeERERUQ9jOHIRd+cd8dIaERFRT2M4ciGTQzkpm4iIqKcxHLmQ1nB04moNmk2cd0RERNQTGI5cyKPDBlnmHZ0p57wjIiKinsBw5EI474iIiKjnMRy5GM47IiIi6lldCkdbtmxBSEgItFot9Ho9Dh061GF9Xl4e9Ho9tFotQkNDsW3bNpuazMxMREREQKPRICIiAllZWVbr6+vrkZKSAp1OBw8PD8TGxuL48eNWNTdu3MDzzz+P4OBgeHp6Yvr06bh06ZJVzbRp06BQKKxeCxcu7MowOAXnHREREfUsh8NRRkYGUlJSsH79ehQVFSEuLg4zZsxAaWmp3fqSkhLMnDkTcXFxKCoqwrp167BixQpkZmZaavLz87FgwQIkJSXh9OnTSEpKwvz581FQUGCpWbJkCXJycpCeno4zZ84gISEB8fHxKC8vBwCICObOnYsrV65g9+7dKCoqgk6nQ3x8PG7dumXV09KlS1FRUWF5vfvuu44Og9NYzzuqdXY7RERE/Y84aOLEiZKcnGy1LDw8XNasWWO3/tVXX5Xw8HCrZS+88IJMnjzZ8vP8+fNl+vTpVjWJiYmycOFCERG5ffu2KJVK2bt3r1VNZGSkrF+/XkRELly4IADk7NmzlvUmk0n8/Pzk/ffftyybOnWqvPTSS508WlsGg0EAiMFg6PI2HlRy+gnRrd4r7/z1ktN6ICIiciWOfH47dOaoubkZhYWFSEhIsFqekJCAI0eO2H1Pfn6+TX1iYiJOnDgBo9HYYU3rNk0mE8xmM7RarVWNh4cHDh8+DABoamoCAKsapVIJtVptqWn10Ucfwd/fH2PHjsXLL7+M+vr6do+5qakJdXV1Vi9nixnNeUdEREQ9xaFwVF1dDbPZjICAAKvlAQEBqKystPueyspKu/UmkwnV1dUd1rRu09vbGzExMdiwYQOuX78Os9mM7du3o6CgABUVFQCA8PBw6HQ6rF27FjU1NWhubkZqaioqKystNQDw3HPPYceOHcjNzcXrr7+OzMxMzJs3r91j3rhxI3x9fS2vkSNHdnK0eg7nHREREfWcLk3IVigUVj+LiM2y+9Xfu/x+20xPT4eIYPjw4dBoNNi0aRMWLVoEpVIJAFCpVMjMzMTFixfh5+cHT09P5ObmYsaMGZYa4O58o/j4eIwbNw4LFy7Exx9/jIMHD+LkyZN2e1+7di0MBoPlVVZW1tHQ9ArOOyIiIuo5DoUjf39/KJVKm7NEVVVVNmd+WgUGBtqtd3d3x5AhQzqsabvN0aNHIy8vDw0NDSgrK8OxY8dgNBoREhJiqdHr9Th16hRqa2tRUVGB/fv34+bNm1Y194qKioJKpbL5VlsrjUYDHx8fq5ezKRQKTA69e7+j/Mu8tEZERNSdHApHarUaer0eOTk5VstzcnIQGxtr9z0xMTE29dnZ2YiOjoZKpeqwxt42vby8EBQUhJqaGhw4cABz5syxqfH19cXQoUNx6dIlnDhxwm5Nq3PnzsFoNCIoKKjdmr7o2/sdfe3kToiIiPoXd0ffsGrVKiQlJSE6OhoxMTF47733UFpaiuTkZAB3L0OVl5fjww8/BAAkJyfjnXfewapVq7B06VLk5+cjLS0NO3bssGzzpZdewlNPPYU333wTc+bMwe7du3Hw4EGridQHDhyAiCAsLAxffvklXnnlFYSFheEnP/mJpWbXrl0YOnQoHn74YZw5cwYvvfQS5s6da5nsffnyZXz00UeYOXMm/P39UVxcjJ///OeYMGECnnzyya6NoJNY5h199TWaTS1Qu/N+nkRERN2iK1+H27x5s+h0OlGr1RIVFSV5eXmWdYsXL5apU6da1efm5sqECRNErVbLqFGjZOvWrTbb3LVrl4SFhYlKpZLw8HDJzMy0Wp+RkSGhoaGiVqslMDBQli1bJrW1tVY1v//972XEiBGiUqnk4Ycfltdee02ampos60tLS+Wpp54SPz8/UavVMnr0aFmxYoXcvHmz08feF77KLyLS0tIiUb/MFt3qvXK8pPP9ExERDUSOfH4rRL6ZHU2dUldXB19fXxgMBqfPP1r20Ul8dqYCLyc8huXPPOrUXoiIiPoyRz6/eS3GhbVOyua8IyIiou7DcOTC7p13RERERA+O4ciFPTJsEIZ4qXHH2IK/Xat1djtERET9AsORC7t7v6O7Z494vyMiIqLuwXDk4izzjkoYjoiIiLoDw5GLaz1zVPhVDZpMZid3Q0RE5PoYjlzcI8MGwX9Q67wjg7PbISIicnkMRy5OoVBgUuujRDjviIiI6IExHPUDluescd4RERHRA2M46gdivpmUfeIq5x0RERE9KIajfmD00LvzjppMLThdxnlHRERED4LhqB+wmnd0hZfWiIiIHgTDUT8Rw3BERETULRiO+gne74iIiKh7MBz1E6OHesF/kAZNphacKq11djtEREQui+Gon7j7nLVvHiVy5Wsnd0NEROS6GI76kcmcd0RERPTAGI76kZjRd8PRydIa3DFy3hEREVFXMBz1I6H+Xhjqrfnmfke1zm6HiIjIJTEc9SN35x3dPXuUz0trREREXcJw1M98Oymb4YiIiKgrGI76mdYzRydLaznviIiIqAsYjvqZUH8vDPPWoNnUglOcd0REROQwhqN+xmre0WVeWiMiInIUw1E/xPsdERERdR3DUT/UOim7qIzzjoiIiBzFcNQPhbSZd1TE56wRERE5hOGoH1IoFJa7ZfN+R0RERI5hOOqnOO+IiIioaxiO+qnWcHSK9zsiIiJyCMNRPzVqiCcCfDRoNrfgZGmNs9shIiJyGQxH/ZRCoUBM66U13u+IiIio07oUjrZs2YKQkBBotVro9XocOnSow/q8vDzo9XpotVqEhoZi27ZtNjWZmZmIiIiARqNBREQEsrKyrNbX19cjJSUFOp0OHh4eiI2NxfHjx61qbty4geeffx7BwcHw9PTE9OnTcenSJauapqYmvPjii/D394eXlxdmz56Na9eudWUY+rxv5x197eROiIiIXIfD4SgjIwMpKSlYv349ioqKEBcXhxkzZqC0tNRufUlJCWbOnIm4uDgUFRVh3bp1WLFiBTIzMy01+fn5WLBgAZKSknD69GkkJSVh/vz5KCgosNQsWbIEOTk5SE9Px5kzZ5CQkID4+HiUl5cDAEQEc+fOxZUrV7B7924UFRVBp9MhPj4et27dsmwnJSUFWVlZ2LlzJw4fPoyGhgbMmjULZnP/m5djmXdUVovG5v53fERERD1CHDRx4kRJTk62WhYeHi5r1qyxW//qq69KeHi41bIXXnhBJk+ebPl5/vz5Mn36dKuaxMREWbhwoYiI3L59W5RKpezdu9eqJjIyUtavXy8iIhcuXBAAcvbsWct6k8kkfn5+8v7774uISG1trahUKtm5c6elpry8XNzc3GT//v2dOn6DwSAAxGAwdKremVpaWmTyrw6KbvVe+eLSP5zdDhERkdM48vnt0Jmj5uZmFBYWIiEhwWp5QkICjhw5Yvc9+fn5NvWJiYk4ceIEjEZjhzWt2zSZTDCbzdBqtVY1Hh4eOHz4MIC7l8sAWNUolUqo1WpLTWFhIYxGo9W+goODMW7cuHb7b2pqQl1dndXLVVg9Z41f6SciIuoUh8JRdXU1zGYzAgICrJYHBASgsrLS7nsqKyvt1ptMJlRXV3dY07pNb29vxMTEYMOGDbh+/TrMZjO2b9+OgoICVFRUAADCw8Oh0+mwdu1a1NTUoLm5GampqaisrLTUVFZWQq1WY/DgwZ3uf+PGjfD19bW8Ro4c2Zmh6jNaHyXC+x0RERF1TpcmZCsUCqufRcRm2f3q711+v22mp6dDRDB8+HBoNBps2rQJixYtglKpBACoVCpkZmbi4sWL8PPzg6enJ3JzczFjxgxLTXs66n/t2rUwGAyWV1lZWYfb6ms474iIiMgxDoUjf39/KJVKm7MsVVVVNmd+WgUGBtqtd3d3x5AhQzqsabvN0aNHIy8vDw0NDSgrK8OxY8dgNBoREhJiqdHr9Th16hRqa2tRUVGB/fv34+bNm5aawMBANDc3o6ampsN9taXRaODj42P1ciUP+3ki2FcLo1l4vyMiIqJOcCgcqdVq6PV65OTkWC3PyclBbGys3ffExMTY1GdnZyM6OhoqlarDGnvb9PLyQlBQEGpqanDgwAHMmTPHpsbX1xdDhw7FpUuXcOLECUuNXq+HSqWy2ldFRQXOnj3bbv+uzmreEe93REREdH+OzvbeuXOnqFQqSUtLk+LiYklJSREvLy+5evWqiIisWbNGkpKSLPVXrlwRT09PWblypRQXF0taWpqoVCr5+OOPLTVffPGFKJVKSU1NlfPnz0tqaqq4u7vL0aNHLTX79++Xffv2yZUrVyQ7O1siIyNl4sSJ0tzcbKn5n//5H/n888/l8uXL8umnn4pOp5N58+ZZ9Z+cnCwjRoyQgwcPysmTJ+WZZ56RyMhIMZlMnTp+V/q2WquMY6WiW71X5m35wtmtEBEROYUjn98OhyMRkc2bN4tOpxO1Wi1RUVGSl5dnWbd48WKZOnWqVX1ubq5MmDBB1Gq1jBo1SrZu3WqzzV27dklYWJioVCoJDw+XzMxMq/UZGRkSGhoqarVaAgMDZdmyZVJbW2tV8/vf/15GjBghKpVKHn74YXnttdekqanJqqaxsVGWL18ufn5+4uHhIbNmzZLS0tJOH7srhqOvqm+JbvVeeWTdZ3KryejsdoiIiHqdI5/fCpFvZkdTp9TV1cHX1xcGg8Fl5h+JCKa8+TnKaxuR/tOJiHt0qLNbIiIi6lWOfH7z2WoDgEKhwCR+pZ+IiKhTGI4GCD5njYiIqHMYjgaImG/C0emyWtxqMjm5GyIior6L4WiAGOnnieEPecDUIij8ivc7IiIiag/D0QDy7aU1zjsiIiJqD8PRABIzmg+hJSIiuh+GowFkUsjdb6z97ZqB846IiIjawXA0gIz088SIwR4wtwhOcN4RERGRXQxHA0wMn7NGRETUIYajAYaTsomIiDrGcDTAtN4p+0y5AQ2cd0RERGSD4WiAGTHYEyP9vpl3dJV3yyYiIroXw9EAZJl3xEtrRERENhiOBiA+Z42IiKh9DEcDUGs4OltuQP0do5O7ISIi6lsYjgag4Ic8oBvi+c28I97viIiIqC2GowFqcgi/0k9ERGQPw9EANXn03a/0c1I2ERGRNYajAartvKM6zjsiIiKyYDgaoIJ8PTBqiCdaBLzfERERURsMRwNYzGg+Z42IiOheDEcDGO93REREZIvhaABrDUfnrhtgaOS8IyIiIoDhaEAL8NEi1N8LLQIcL+HZIyIiIoDhaMCbFMr7HREREbXFcDTAWSZlMxwREREBYDga8CaH3L0ZZHFFHQy3Oe+IiIiI4WiAG+ajRehQL4gAx3i/IyIiIoYjAmJCeb8jIiKiVgxH1OZ+RwxHREREDEeESaF35x2dr6xD7e1mJ3dDRETkXAxHhGHeWjwybBBEgALe74iIiAa4LoWjLVu2ICQkBFqtFnq9HocOHeqwPi8vD3q9HlqtFqGhodi2bZtNTWZmJiIiIqDRaBAREYGsrCyr9fX19UhJSYFOp4OHhwdiY2Nx/Phxq5qGhgYsX74cI0aMgIeHB8aMGYOtW7da1UybNg0KhcLqtXDhwq4MQ78y+ZuzR7y0RkREA53D4SgjIwMpKSlYv349ioqKEBcXhxkzZqC0tNRufUlJCWbOnIm4uDgUFRVh3bp1WLFiBTIzMy01+fn5WLBgAZKSknD69GkkJSVh/vz5KCgosNQsWbIEOTk5SE9Px5kzZ5CQkID4+HiUl5dbalauXIn9+/dj+/btOH/+PFauXIkXX3wRu3fvtupp6dKlqKiosLzeffddR4eh34kJ9QfASdlEREQKERFH3jBp0iRERUVZnZEZM2YM5s6di40bN9rUr169Gnv27MH58+cty5KTk3H69Gnk5+cDABYsWIC6ujrs27fPUjN9+nQMHjwYO3bsQGNjI7y9vbF79248++yzlprx48dj1qxZeOONNwAA48aNw4IFC/D6669bavR6PWbOnIkNGzYAuHvmaPz48Xj77bcdOWyLuro6+Pr6wmAwwMfHp0vb6IuqG5oQ/cZBAEDR6/+MwV5qJ3dERETUfRz5/HbozFFzczMKCwuRkJBgtTwhIQFHjhyx+578/Hyb+sTERJw4cQJGo7HDmtZtmkwmmM1maLVaqxoPDw8cPnzY8vOUKVOwZ88elJeXQ0Tw+eef4+LFi0hMTLR630cffQR/f3+MHTsWL7/8Murr69s95qamJtTV1Vm9+iP/QRo8OmwQAKCghGePiIho4HIoHFVXV8NsNiMgIMBqeUBAACorK+2+p7Ky0m69yWRCdXV1hzWt2/T29kZMTAw2bNiA69evw2w2Y/v27SgoKEBFRYXlPZs2bUJERARGjBgBtVqN6dOnY8uWLZgyZYql5rnnnsOOHTuQm5uL119/HZmZmZg3b167x7xx40b4+vpaXiNHjuzESLmm1keJHL3CSdlERDRwdWlCtkKhsPpZRGyW3a/+3uX322Z6ejpEBMOHD4dGo8GmTZuwaNEiKJVKS82mTZtw9OhR7NmzB4WFhXjrrbfws5/9DAcPHrTULF26FPHx8Rg3bhwWLlyIjz/+GAcPHsTJkyft9r527VoYDAbLq6ysrN3jdHWTeTNIIiIiuDtS7O/vD6VSaXOWqKqqyubMT6vAwEC79e7u7hgyZEiHNW23OXr0aOTl5eHWrVuoq6tDUFAQFixYgJCQEABAY2Mj1q1bh6ysLMu8pMcffxynTp3Cb37zG8THx9vtLyoqCiqVCpcuXUJUVJTNeo1GA41G09Gw9BuTvnnO2oUb9bjZ0IQhgwbGcRMREbXl0JkjtVoNvV6PnJwcq+U5OTmIjY21+56YmBib+uzsbERHR0OlUnVYY2+bXl5eCAoKQk1NDQ4cOIA5c+YAAIxGI4xGI9zcrA9JqVSipaWl3WM6d+4cjEYjgoKC2q0ZKIYM0iAswBsAcIz3OyIiogHKoTNHALBq1SokJSUhOjoaMTExeO+991BaWork5GQAdy9DlZeX48MPPwRw95tp77zzDlatWoWlS5ciPz8faWlp2LFjh2WbL730Ep566im8+eabmDNnDnbv3o2DBw9aTbY+cOAARARhYWH48ssv8corryAsLAw/+clPAAA+Pj6YOnUqXnnlFXh4eECn0yEvLw8ffvghfvvb3wIALl++jI8++ggzZ86Ev78/iouL8fOf/xwTJkzAk08+2fVR7EdiRg/BhRv1yL9yEzO+w8BIREQDkHTB5s2bRafTiVqtlqioKMnLy7OsW7x4sUydOtWqPjc3VyZMmCBqtVpGjRolW7dutdnmrl27JCwsTFQqlYSHh0tmZqbV+oyMDAkNDRW1Wi2BgYGybNkyqa2ttaqpqKiQ559/XoKDg0Wr1UpYWJi89dZb0tLSIiIipaWl8tRTT4mfn5+o1WoZPXq0rFixQm7evNnpYzcYDAJADAZDp9/jSvaduS661Xvln3+b6+xWiIiIuo0jn98O3+dooOuv9zlqVXOrGRM23L3EeeK1ePhz3hEREfUDPXafI+r/BnupER54d95RAb/ST0REAxDDEdmwfKX/SrWTOyEiIup9DEdkgzeDJCKigYzhiGxMCvGDQgF8WdWAqvo7zm6HiIioVzEckY2HPNUYE3h3shrnHRER0UDDcER2tc47OnqFjxIhIqKBheGI7Gqdd5TPcERERAMMwxHZNXHU3XlHV/5xC1V1nHdEREQDB8MR2eXrqcLY4Lvzjnj2iIiIBhKGI2rX5BB+pZ+IiAYehiNq17f3O+KZIyIiGjgYjqhdT4T4wU0BlFTfQqWB846IiGhgYDiidvloVRg33BcAzx4REdHAwXBEHeL9joiIaKBhOKIOxYTyfkdERDSwMBxRh6JHDYbSTYGvbt7G9dpGZ7dDRETU4xiOqEPenHdEREQDDMMR3dfkUD8AQP5lhiMiIur/GI7ovlrnHR0tYTgiIqL+j+GI7it6lB+UbgqUfd2IazW3nd0OERFRj2I4ovsapHHH4yNa5x3xUSJERNS/MRxRp7Te74jzjoiIqL9jOKJOieHNIImIaIBgOKJO0esGw91NgfLaRpR9zXlHRETUfzEcUad4adwROfIhALxbNhER9W8MR9Rprfc7Osp5R0RE1I8xHFGnxYT6A7h75khEnNwNERFRz2A4ok7T6wZDpVSgwnAHpZx3RERE/RTDEXWah1qJ8d/MO+K31oiIqL9iOCKH8H5HRETU3zEckUNa73fEeUdERNRfMRyRQ6J0g6FWuuFGXROu3uS8IyIi6n+6FI62bNmCkJAQaLVa6PV6HDp0qMP6vLw86PV6aLVahIaGYtu2bTY1mZmZiIiIgEajQUREBLKysqzW19fXIyUlBTqdDh4eHoiNjcXx48etahoaGrB8+XKMGDECHh4eGDNmDLZu3WpV09TUhBdffBH+/v7w8vLC7Nmzce3ata4Mw4CkVSkx/uGHAPDSGhER9U8Oh6OMjAykpKRg/fr1KCoqQlxcHGbMmIHS0lK79SUlJZg5cybi4uJQVFSEdevWYcWKFcjMzLTU5OfnY8GCBUhKSsLp06eRlJSE+fPno6CgwFKzZMkS5OTkID09HWfOnEFCQgLi4+NRXl5uqVm5ciX279+P7du34/z581i5ciVefPFF7N6921KTkpKCrKws7Ny5E4cPH0ZDQwNmzZoFs9ns6FAMWHyUCBER9WvioIkTJ0pycrLVsvDwcFmzZo3d+ldffVXCw8Otlr3wwgsyefJky8/z58+X6dOnW9UkJibKwoULRUTk9u3bolQqZe/evVY1kZGRsn79esvPY8eOlV/+8pdWNVFRUfLaa6+JiEhtba2oVCrZuXOnZX15ebm4ubnJ/v37OzzuVgaDQQCIwWDoVH1/dOTLatGt3ivRb+RIS0uLs9shIiK6L0c+vx06c9Tc3IzCwkIkJCRYLU9ISMCRI0fsvic/P9+mPjExESdOnIDRaOywpnWbJpMJZrMZWq3WqsbDwwOHDx+2/DxlyhTs2bMH5eXlEBF8/vnnuHjxIhITEwEAhYWFMBqNVvsKDg7GuHHj2u2/qakJdXV1Vq+BbsLDD0Ht7oZ/1DfhSvUtZ7dDRETUrRwKR9XV1TCbzQgICLBaHhAQgMrKSrvvqaystFtvMplQXV3dYU3rNr29vRETE4MNGzbg+vXrMJvN2L59OwoKClBRUWF5z6ZNmxAREYERI0ZArVZj+vTp2LJlC6ZMmWLZj1qtxuDBgzvd/8aNG+Hr62t5jRw58n7D1O9pVUpEcd4RERH1U12akK1QKKx+FhGbZferv3f5/baZnp4OEcHw4cOh0WiwadMmLFq0CEql0lKzadMmHD16FHv27EFhYSHeeust/OxnP8PBgwc7PJ6O+l+7di0MBoPlVVZW1uG2Boq2jxIhIiLqT9wdKfb394dSqbQ5y1JVVWVz5qdVYGCg3Xp3d3cMGTKkw5q22xw9ejTy8vJw69Yt1NXVISgoCAsWLEBISAgAoLGxEevWrUNWVhaeffZZAMDjjz+OU6dO4Te/+Q3i4+MRGBiI5uZm1NTUWJ09qqqqQmxsrN3+NRoNNBpNZ4ZnQIkZPQS/OwgUfHO/o47CMRERkStx6MyRWq2GXq9HTk6O1fKcnJx2w0VMTIxNfXZ2NqKjo6FSqTqssbdNLy8vBAUFoaamBgcOHMCcOXMAAEajEUajEW5u1oekVCrR0tICANDr9VCpVFb7qqiowNmzZ9vtn+yLHOkLjbsbqhua8WVVg7PbISIi6j6OzvbeuXOnqFQqSUtLk+LiYklJSREvLy+5evWqiIisWbNGkpKSLPVXrlwRT09PWblypRQXF0taWpqoVCr5+OOPLTVffPGFKJVKSU1NlfPnz0tqaqq4u7vL0aNHLTX79++Xffv2yZUrVyQ7O1siIyNl4sSJ0tzcbKmZOnWqjB07Vj7//HO5cuWK/OEPfxCtVitbtmyx1CQnJ8uIESPk4MGDcvLkSXnmmWckMjJSTCZTp46f31b71qL380W3eq98eKTE2a0QERF1yJHPb4fDkYjI5s2bRafTiVqtlqioKMnLy7OsW7x4sUydOtWqPjc3VyZMmCBqtVpGjRolW7dutdnmrl27JCwsTFQqlYSHh0tmZqbV+oyMDAkNDRW1Wi2BgYGybNkyqa2ttaqpqKiQ559/XoKDg0Wr1UpYWJi89dZbVl83b2xslOXLl4ufn594eHjIrFmzpLS0tNPHznD0rU0HL4pu9V751+0nnN0KERFRhxz5/FaI8AFZjqirq4Ovry8MBgN8fHyc3Y5Tnbj6Nf7ftnz4ealR+Fo85x0REVGf5cjnN5+tRl32+IiH4KFS4utbzbh4g/OOiIiof2A4oi5Tu7shetTdb/3lX652cjdERETdg+GIHshky3PWvnZyJ0RERN2D4YgeiCUcldxESwunrxERketjOKIH8vgIX3iqlai9bcSFG/XOboeIiOiBMRzRA1Ep3fDEKD8AfM4aERH1DwxH9MBaL63xOWtERNQfMBzRA4sZfTccHSv5mvOOiIjI5TEc0QMbF+yDQRp3GBqNKK6oc3Y7RERED4ThiB6Yu9INT3xzv6OjvLRGREQujuGIukXrpTWGIyIicnUMR9QtWidlF5R8DTPnHRERkQtjOKJuMTbYF95ad9TfMeHcdYOz2yEiIuoyhiPqFko3BSaF3L3fES+tERGRK2M4om5jud8RbwZJREQujOGIuk1rODp+tQYmc4uTuyEiIuoahiPqNhFBPvD1UKGhyYSz13m/IyIick0MR9Rt3NwUmBjC56wREZFrYziibhXD56wREZGLYziibtV6M8gTV7+GkfOOiIjIBTEcUbcKC/DGYE8Vbjeb8bdrvN8RERG5HoYj6lZubgpMCuGjRIiIyHUxHFG343PWiIjIlTEcUbdrvd/Rias1aDZx3hEREbkWhiPqdo8FDMIQLzUajWacvlbr7HaIiIgcwnBE3U6hUPBRIkRE5LIYjqhHTA7lQ2iJiMg1MRxRj2idlF34VQ2aTGYnd0NERNR5DEfUI0YPHQT/QRo0mVpQVFrr7HaIiIg6jeGIesTdeUe8tEZERK6H4Yh6TOulNU7KJiIiV8JwRD2m9SG0RaW1uGPkvCMiInINDEfUY0L8vTDMW4NmcwtOltY4ux0iIqJO6VI42rJlC0JCQqDVaqHX63Ho0KEO6/Py8qDX66HVahEaGopt27bZ1GRmZiIiIgIajQYRERHIysqyWl9fX4+UlBTodDp4eHggNjYWx48ft6pRKBR2X7/+9a8tNdOmTbNZv3Dhwq4MA92HQqH49lEivLRGREQuwuFwlJGRgZSUFKxfvx5FRUWIi4vDjBkzUFpaare+pKQEM2fORFxcHIqKirBu3TqsWLECmZmZlpr8/HwsWLAASUlJOH36NJKSkjB//nwUFBRYapYsWYKcnBykp6fjzJkzSEhIQHx8PMrLyy01FRUVVq8PPvgACoUC3//+9616Wrp0qVXdu+++6+gwUCe1XlrL56RsIiJyEQoREUfeMGnSJERFRWHr1q2WZWPGjMHcuXOxceNGm/rVq1djz549OH/+vGVZcnIyTp8+jfz8fADAggULUFdXh3379llqpk+fjsGDB2PHjh1obGyEt7c3du/ejWeffdZSM378eMyaNQtvvPGG3V7nzp2L+vp6/OUvf7EsmzZtGsaPH4+3337bkcO2qKurg6+vLwwGA3x8fLq0jYHkq5u3MPXXuVApFfjbLxLhoVY6uyUiIhqAHPn8dujMUXNzMwoLC5GQkGC1PCEhAUeOHLH7nvz8fJv6xMREnDhxAkajscOa1m2aTCaYzWZotVqrGg8PDxw+fNjufm/cuIHPPvsMP/3pT23WffTRR/D398fYsWPx8ssvo76+vt1jbmpqQl1dndWLOu9hP08E+WphNAsKv+K8IyIi6vscCkfV1dUwm80ICAiwWh4QEIDKykq776msrLRbbzKZUF1d3WFN6za9vb0RExODDRs24Pr16zCbzdi+fTsKCgpQUVFhd79//OMf4e3tjXnz5lktf+6557Bjxw7k5ubi9ddfR2Zmpk1NWxs3boSvr6/lNXLkyHZryZZCobBcWjtyudrJ3RAREd1flyZkKxQKq59FxGbZ/ervXX6/baanp0NEMHz4cGg0GmzatAmLFi2CUmn/Ms0HH3yA5557zuZs09KlSxEfH49x48Zh4cKF+Pjjj3Hw4EGcPHnS7nbWrl0Lg8FgeZWVlbV7nGTfk4/4AwDyLv7DyZ0QERHdn0PhyN/fH0ql0uYsUVVVlc2Zn1aBgYF2693d3TFkyJAOa9puc/To0cjLy0NDQwPKyspw7NgxGI1GhISE2Ozz0KFDuHDhApYsWXLfY4qKioJKpcKlS5fsrtdoNPDx8bF6kWOmhg2FQgGcu16HG3V3nN0OERFRhxwKR2q1Gnq9Hjk5OVbLc3JyEBsba/c9MTExNvXZ2dmIjo6GSqXqsMbeNr28vBAUFISamhocOHAAc+bMsalJS0uDXq9HZGTkfY/p3LlzMBqNCAoKum8tdY3/IA0eH/EQACDvAs8eERFR3+bwZbVVq1bhv//7v/HBBx/g/PnzWLlyJUpLS5GcnAzg7mWoH//4x5b65ORkfPXVV1i1ahXOnz+PDz74AGlpaXj55ZctNS+99BKys7Px5ptv4u9//zvefPNNHDx4ECkpKZaaAwcOYP/+/SgpKUFOTg6efvpphIWF4Sc/+YlVf3V1ddi1a5fds0aXL1/GL3/5S5w4cQJXr17Fn//8Z/zgBz/AhAkT8OSTTzo6FOSAp8OGAgD++vcqJ3dCRER0H9IFmzdvFp1OJ2q1WqKioiQvL8+ybvHixTJ16lSr+tzcXJkwYYKo1WoZNWqUbN261Wabu3btkrCwMFGpVBIeHi6ZmZlW6zMyMiQ0NFTUarUEBgbKsmXLpLa21mY77777rnh4eNhdV1paKk899ZT4+fmJWq2W0aNHy4oVK+TmzZudPnaDwSAAxGAwdPo9JHKqtEZ0q/fK2P9vvzQZzc5uh4iIBhhHPr8dvs/RQMf7HHVNS4tg4q8OorqhGX9aOgmxo/2d3RIREQ0gPXafI6KucnNTYOpjwwAAuZx3REREfRjDEfWap8M574iIiPo+hiPqNXGPDoXSTYEvqxpQ9vVtZ7dDRERkF8MR9RpfDxX0usEAgNwLPHtERER9E8MR9aqnw+7OO+KlNSIi6qsYjqhXtc47OnL5Ju4YzU7uhoiIyBbDEfWqsABvBPtq0WRqQf6Vm85uh4iIyAbDEfUqhUKBaeHffKWfl9aIiKgPYjiiXmeZd3ShCrwHKRER9TUMR9TrnnxkCNTubij7uhEXbtQ7ux0iIiIrDEfU6zzV7pj22N2J2Z8WXXdyN0RERNYYjsgp5kUNBwDsPlWOlhZeWiMior6D4Yic4unwYfDRuqPCcAdHS/itNSIi6jsYjsgpNO5KPPt4MAAg62S5k7shIiL6FsMROc33Jty9tLbvbCVvCElERH0GwxE5TbRuMEYM9kBDkwk5xTec3Q4REREAhiNyIjc3BeaOv3v26NMiXlojIqK+geGInGruN5fW8i7+AzcbmpzcDREREcMROdkjwwbh8RG+MLUI9v6twtntEBERMRyR87VOzP6El9aIiKgPYDgip/tuZDCUbgqcLqvFlX80OLsdIiIa4BiOyOn8B2nw1KP+ADgxm4iInI/hiPqE1onZWafKIcLHiRARkfMwHFGfkBARCC+1EmVfN6Lwqxpnt0NERAMYwxH1CR5qJaaPCwIAZPHSGhERORHDEfUZ86LuXlrb+7cKNJn4OBEiInIOhiPqMyaHDkGAjwaGRiNyL/zD2e0QEdEAxXBEfYayzeNEsk7y0hoRETkHwxH1Ka3fWvvr36tguG10cjdERDQQMRxRnzImyAfhgd5oNrfgszN8nAgREfU+hiPqc1onZmcVXXNyJ0RENBAxHFGfMztyOBQK4PjVGpR9fdvZ7RAR0QDTpXC0ZcsWhISEQKvVQq/X49ChQx3W5+XlQa/XQ6vVIjQ0FNu2bbOpyczMREREBDQaDSIiIpCVlWW1vr6+HikpKdDpdPDw8EBsbCyOHz9uVaNQKOy+fv3rX1tqmpqa8OKLL8Lf3x9eXl6YPXs2rl3jGYq+JNBXiydH83EiRETkHA6Ho4yMDKSkpGD9+vUoKipCXFwcZsyYgdLSUrv1JSUlmDlzJuLi4lBUVIR169ZhxYoVyMzMtNTk5+djwYIFSEpKwunTp5GUlIT58+ejoKDAUrNkyRLk5OQgPT0dZ86cQUJCAuLj41Fe/u2HZ0VFhdXrgw8+gEKhwPe//31LTUpKCrKysrBz504cPnwYDQ0NmDVrFsxm3lenL7E8TqSIjxMhIqJeJg6aOHGiJCcnWy0LDw+XNWvW2K1/9dVXJTw83GrZCy+8IJMnT7b8PH/+fJk+fbpVTWJioixcuFBERG7fvi1KpVL27t1rVRMZGSnr169vt9c5c+bIM888Y/m5trZWVCqV7Ny507KsvLxc3NzcZP/+/e1upy2DwSAAxGAwdKqeuqb+jlHCXvuz6FbvlaLSGme3Q0RELs6Rz2+Hzhw1NzejsLAQCQkJVssTEhJw5MgRu+/Jz8+3qU9MTMSJEydgNBo7rGndpslkgtlshlartarx8PDA4cOH7e73xo0b+Oyzz/DTn/7UsqywsBBGo9FqX8HBwRg3bly7/Tc1NaGurs7qRT1vkMYdCRGBAHhpjYiIepdD4ai6uhpmsxkBAQFWywMCAlBZWWn3PZWVlXbrTSYTqqurO6xp3aa3tzdiYmKwYcMGXL9+HWazGdu3b0dBQQEqKux/3fuPf/wjvL29MW/ePKte1Go1Bg8e3On+N27cCF9fX8tr5MiRduuo+33vm2+t/e/p6zCaW5zcDRERDRRdmpCtUCisfhYRm2X3q793+f22mZ6eDhHB8OHDodFosGnTJixatAhKpdLuPj/44AM899xzNmeb7Omo/7Vr18JgMFheZWVl990edY+4R/zhP0iNm7eacegSHydCRES9w6Fw5O/vD6VSaXOWpaqqyubMT6vAwEC79e7u7hgyZEiHNW23OXr0aOTl5aGhoQFlZWU4duwYjEYjQkJCbPZ56NAhXLhwAUuWLLHppbm5GTU1NZ3uX6PRwMfHx+pFvcNd6YbvRgYDAD7h40SIiKiXOBSO1Go19Ho9cnJyrJbn5OQgNjbW7ntiYmJs6rOzsxEdHQ2VStVhjb1tenl5ISgoCDU1NThw4ADmzJljU5OWlga9Xo/IyEir5Xq9HiqVympfFRUVOHv2bLv9k3PNmzACAJBTfAN1d/g4ESIi6gWOzvbeuXOnqFQqSUtLk+LiYklJSREvLy+5evWqiIisWbNGkpKSLPVXrlwRT09PWblypRQXF0taWpqoVCr5+OOPLTVffPGFKJVKSU1NlfPnz0tqaqq4u7vL0aNHLTX79++Xffv2yZUrVyQ7O1siIyNl4sSJ0tzcbDMb3dPTU7Zu3Wq3/+TkZBkxYoQcPHhQTp48Kc8884xERkaKyWTq1PHz22q9q6WlRf7prVzRrd4rGcdLnd0OERG5KEc+vx0ORyIimzdvFp1OJ2q1WqKioiQvL8+ybvHixTJ16lSr+tzcXJkwYYKo1WoZNWqU3eCya9cuCQsLE5VKJeHh4ZKZmWm1PiMjQ0JDQ0WtVktgYKAsW7ZMamtrbbbz7rvvioeHh911IiKNjY2yfPly8fPzEw8PD5k1a5aUlnb+Q5fhqPe989dLolu9Vxa+m+/sVoiIyEU58vmtEOEd9hxRV1cHX19fGAwGzj/qJddqbmPKm59DoQC+WP0Mgh/ycHZLRETkYhz5/Oaz1ajPGzHYE5NC/CAC7D513dntEBFRP8dwRC7he5bHiVzj40SIiKhHMRyRS5jxnSCo3d1w8UYDzl3nXcqJiKjnMByRS/D1UOGfx9y9FxUfJ0JERD2J4YhcxtxvLq3tPn0dJj5OhIiIegjDEbmMqY8NxWBPFf5R34QvLt90djtERNRPMRyRy1C7f/s4EV5aIyKinsJwRC6l9Vtr+89W4laTycndEBFRf8RwRC5l/MiHEOLvhUajGQfOVd7/DURERA5iOCKXolAoMHd86z2PeGmNiIi6H8MRuZzWS2tffFmNG3V3nNwNERH1NwxH5HIeHuKJaN1gtAiwh48TISKibsZwRC6p9Z5Hn/DSGhERdTOGI3JJsx4PglrphvMVdfh7JR8nQkRE3YfhiFzSQ55qPB0+FAAnZhMRUfdiOCKX1Toxe3fRdZhbxMndEBFRf8FwRC7r6fBh8PVQobLuDvL5OBEiIuomDEfksjTuSsz+5nEifzr2lZO7ISKi/oLhiFzajybrAAAHzt1ApYH3PCIiogfHcEQuLSzQGxND/GBuEew4VursdoiIqB9gOCKXl/TN2aMdx0phNLc4uRsiInJ1DEfk8hLHBsJ/kAZV9U3IKb7h7HaIiMjFMRyRy1O7u+GHE0cCANLzOTGbiIgeDMMR9Qs/nPgw3BRA/pWb+LKq3tntEBGRC2M4on4h+CEPxI8JAABsP8qJ2URE1HUMR9RvJMXcnZidWXgNt5pMTu6GiIhcFcMR9RtPjvZHiL8X6ptM2H3qurPbISIiF8VwRP2Gm5sCz016GADwxyNX0cLnrRERURcwHFG/8gP9SHhr3HHhRj2yisqd3Q4REbkghiPqV3w9VfjZ048AAH6TfQGNzWYnd0RERK6G4Yj6nZ88OQrDH/JAheEOPviixNntEBGRi2E4on5Hq1Li1elhAIAtn3+Jf9Q3ObkjIiJyJV0KR1u2bEFISAi0Wi30ej0OHTrUYX1eXh70ej20Wi1CQ0Oxbds2m5rMzExERERAo9EgIiICWVlZVuvr6+uRkpICnU4HDw8PxMbG4vjx4zbbOX/+PGbPng1fX194e3tj8uTJKC399r4306ZNg0KhsHotXLiwK8NAfdh3Hw/G4yN8cavZjN//5aKz2yEiIhficDjKyMhASkoK1q9fj6KiIsTFxWHGjBlWAaStkpISzJw5E3FxcSgqKsK6deuwYsUKZGZmWmry8/OxYMECJCUl4fTp00hKSsL8+fNRUFBgqVmyZAlycnKQnp6OM2fOICEhAfHx8Sgv/3bS7eXLlzFlyhSEh4cjNzcXp0+fxuuvvw6tVmvV09KlS1FRUWF5vfvuu44OA/Vxbm4KrJs5BgCw41gZ75pNRESdphARh77vPGnSJERFRWHr1q2WZWPGjMHcuXOxceNGm/rVq1djz549OH/+vGVZcnIyTp8+jfz8fADAggULUFdXh3379llqpk+fjsGDB2PHjh1obGyEt7c3du/ejWeffdZSM378eMyaNQtvvPEGAGDhwoVQqVRIT09vt/9p06Zh/PjxePvttx05bIu6ujr4+vrCYDDAx8enS9ug3rP0wxPIKb6B+DHD8N+Ln3B2O0RE5CSOfH47dOaoubkZhYWFSEhIsFqekJCAI0eO2H1Pfn6+TX1iYiJOnDgBo9HYYU3rNk0mE8xms80ZIA8PDxw+fBgA0NLSgs8++wyPPfYYEhMTMWzYMEyaNAmffvqpTU8fffQR/P39MXbsWLz88suor2//rEJTUxPq6uqsXuQ61swIh9JNgYPnq3D4UrWz2yEiIhfgUDiqrq6G2WxGQECA1fKAgABUVlbafU9lZaXdepPJhOrq6g5rWrfp7e2NmJgYbNiwAdevX4fZbMb27dtRUFCAiooKAEBVVRUaGhqQmpqK6dOnIzs7G9/73vcwb9485OXlWbb73HPPYceOHcjNzcXrr7+OzMxMzJs3r91j3rhxI3x9fS2vkSNHdnK0qC8YPXQQfvTNjSFXZ/4NhkajkzsiIqK+rksTshUKhdXPImKz7H719y6/3zbT09MhIhg+fDg0Gg02bdqERYsWQalUArh75ggA5syZg5UrV2L8+PFYs2YNZs2aZTUBfOnSpYiPj8e4ceOwcOFCfPzxxzh48CBOnjxpt/e1a9fCYDBYXmVlZe0eJ/VNr0wPx8N+niivbcQvdp91djtERNTHORSO/P39oVQqbc4SVVVV2Zz5aRUYGGi33t3dHUOGDOmwpu02R48ejby8PDQ0NKCsrAzHjh2D0WhESEiIpTd3d3dERERYbWfMmDHtThYHgKioKKhUKly6dMnueo1GAx8fH6sXuZZBGnf8bsF4KN0U+PTUdew+xTtnExFR+xwKR2q1Gnq9Hjk5OVbLc3JyEBsba/c9MTExNvXZ2dmIjo6GSqXqsMbeNr28vBAUFISamhocOHAAc+bMsfT2xBNP4MKFC1b1Fy9ehE6na/eYzp07B6PRiKCgoHZryPXpdYOx/Js7Z7/26VmU1zY6uSMiIuqzxEE7d+4UlUolaWlpUlxcLCkpKeLl5SVXr14VEZE1a9ZIUlKSpf7KlSvi6ekpK1eulOLiYklLSxOVSiUff/yxpeaLL74QpVIpqampcv78eUlNTRV3d3c5evSopWb//v2yb98+uXLlimRnZ0tkZKRMnDhRmpubLTWffPKJqFQqee+99+TSpUvyX//1X6JUKuXQoUMiIvLll1/Kv//7v8vx48elpKREPvvsMwkPD5cJEyaIyWTq1PEbDAYBIAaDwdGhIyczmswy553Dolu9V+ZvOyImc4uzWyIiol7iyOe3w+FIRGTz5s2i0+lErVZLVFSU5OXlWdYtXrxYpk6dalWfm5srEyZMELVaLaNGjZKtW7fabHPXrl0SFhYmKpVKwsPDJTMz02p9RkaGhIaGilqtlsDAQFm2bJnU1tbabCctLU0eeeQR0Wq1EhkZKZ9++qllXWlpqTz11FPi5+cnarVaRo8eLStWrJCbN292+tgZjlxbyT8aZMzr+0S3eq9sy/3S2e0QEVEvceTz2+H7HA10vM+R68s4XorVmWegUiqQ9bMnMW64r7NbIiKiHtZj9zki6g/mR49E4tgAGM2ClIxTuGM0O7slIiLqQxiOaMBRKBTYOO9xDPXW4MuqBmz88/n7v4mIiAYMhiMakPy81PjNDyIBAH/M/wqfX6hyckdERNRXMBzRgDX1saF4PnYUAOCVXX/DzYYm5zZERER9AsMRDWhrZoTjsYBBqG5owppPzoDfTyAiIoYjGtC0KiXeXjABaqUbcopv4E/H2r+bOhERDQwMRzTgRQT74JXEMADAv/9vMc6WG5zcERERORPDERGAn04JQfyYYWg2tSB5eyEMt43ObomIiJyE4YgIgJubAm/9YDxG+nngWk0jVv3PKbS0cP4REdFAxHBE9A1fTxW2PqeH2t0Nf/l7Fbb932Vnt0RERE7AcETUxrjhvvjl7LEAgN8cuIAjl6ud3BEREfU2hiOieyx4YiT+n34EWgR48U9FuFBZ7+yWiIioFzEcEd1DoVBgw5xxGBvsg5u3mrHgvXycKqt1dltERNRLGI6I7PBQK/HRkkkYP/Ih1N424rn3j/ISGxHRAMFwRNSOhzzV+GjJJDz5yBDcajbj+T8cR07xDWe3RUREPYzhiKgDXhp3pC1+AgkRAZZ7IH1wuARmfs2fiKjfYjgiug+tSoktz0VhXtRwmFsEv9xbjHlbvuCdtImI+imGI6JOcFe64Tf/LxIb5oyFt8Ydp68ZMPudw/jl/xajocnk7PaIiKgbMRwRdZKbmwJJMaPwl59PxazHg9AiwAdflCD+rTzsP1sBEV5qIyLqDxiOiBw0zEeLdxZF4Y//MhEP+3misu4OkrefxJI/nkDZ17ed3R4RET0ghiOiLpr62FBkr3wKy59+BCqlAn/5exUSfvd/2JZ3GUZzi7PbIyKiLlIIrwU4pK6uDr6+vjAYDPDx8XF2O9RHfFlVj3VZZ3Gs5GsAwOihXnh1ejgSIgKgUCic3B0RETny+c1w5CCGI2qPiCDzZDl+9efz+PpWMwAgWjcYa2eOgV432MndERENbAxHPYjhiO6n7o4R7+ZdRtrhEtwx3r28Fj8mAN+NDELco0Ph56V2codERAMPw1EPYjiizqo03MHvci5iV2EZWu8ZqVAA3xnui6ceHYqwQG8M0rhjkNYdXmp3eGvd4aVxxyCNO9TunA5IRNSdGI56EMMROerSjXrsKryG/7v4D/y9sr5T71G7u2GQxh0eKiU0Kjdo3JXQuLtB7e4Gt2+mMCmgQOt0JkWbZW1/bkuhaF17d73innV339/2vQqr7bR9r719WS1XWL+no/23nZOluOcPbffRuf3fs6zN/gXAt/+3E4gALdL6T1i59/1392//eNFBj/hmnwKgpUXQIgJzCyCQdo+vvWO0t797e2yr9Vhbj1ukzTHD9r2KNh20N6Yd9WjZLwQt8u3xth3be9/f0fF1dIxtf/etj1Us4y0iaGkz1vf2eu8+7f3OtPu71qbG3PrvVMTqzvmd+Z1p7xg72l9bAljGuPV42zu+9rZnb0zv/n/Adq939/Xt8bZND/c7xo6Or211636Hemuw7OlH2jnyrmE46kEMR/QgbtTdwf9d/Ae++LIalXV30NBkwq0mM+rvmNDQZLRchiMiGshGD/XCX34+rVu36cjnt3u37pmIOhTgo8UPokfiB9Ej7a43mVtwq8mMhmYTGu6Y0Gg0o8loRpOpBU2mFjSbWqz+dgzA5uaT9/4t2nqZ9XukzcLWv2HfW9vuNtuusNr2t/uwWdZOz601bXu43/7v3Y7tPr59j72/ibt9cwrOTaGw/M227XvvHQ/rXm3HSu4Zx7bcFAq4Ke7+rbj1z+31eu/+OnuMbde1PQPYekagdf9t/+Zu7xjtHZtNTQdjAwBKt7vH6OamsOy/ozG9d5+t+23333vbAWlz1hPfHHfb8W4dh64cY3u/6237cFMoLMfb3v7a2+e9vzM2x9jmv9M2h2p15sfN7e7vVOu/c4VC0aXfmbYr2/tvT6lQWPandGvbhe0x2tufTT/37K/t78hgT+fOzWQ4IupD3JVu8PV0g6+nytmtEBENWJz1SURERNQGwxERERFRGwxHRERERG10KRxt2bIFISEh0Gq10Ov1OHToUIf1eXl50Ov10Gq1CA0NxbZt22xqMjMzERERAY1Gg4iICGRlZVmtr6+vR0pKCnQ6HTw8PBAbG4vjx4/bbOf8+fOYPXs2fH194e3tjcmTJ6O0tNSyvqmpCS+++CL8/f3h5eWF2bNn49q1a10ZBiIiIuqHHA5HGRkZSElJwfr161FUVIS4uDjMmDHDKoC0VVJSgpkzZyIuLg5FRUVYt24dVqxYgczMTEtNfn4+FixYgKSkJJw+fRpJSUmYP38+CgoKLDVLlixBTk4O0tPTcebMGSQkJCA+Ph7l5eWWmsuXL2PKlCkIDw9Hbm4uTp8+jddffx1ardZSk5KSgqysLOzcuROHDx9GQ0MDZs2aBbPZ7OhQEBERUT/k8H2OJk2ahKioKGzdutWybMyYMZg7dy42btxoU7969Wrs2bMH58+ftyxLTk7G6dOnkZ+fDwBYsGAB6urqsG/fPkvN9OnTMXjwYOzYsQONjY3w9vbG7t278eyzz1pqxo8fj1mzZuGNN94AACxcuBAqlQrp6el2ezcYDBg6dCjS09OxYMECAMD169cxcuRI/PnPf0ZiYuJ9j5/3OSIiInI9jnx+O3TmqLm5GYWFhUhISLBanpCQgCNHjth9T35+vk19YmIiTpw4AaPR2GFN6zZNJhPMZrPVGSAA8PDwwOHDhwEALS0t+Oyzz/DYY48hMTERw4YNw6RJk/Dpp59a6gsLC2E0Gq32FRwcjHHjxrXbPxEREQ0sDoWj6upqmM1mBAQEWC0PCAhAZWWl3fdUVlbarTeZTKiuru6wpnWb3t7eiImJwYYNG3D9+nWYzWZs374dBQUFqKioAABUVVWhoaEBqampmD59OrKzs/G9730P8+bNQ15enmU/arUagwcP7nT/TU1NqKurs3oRERFR/9WlCdn3PnNFROw+h6Wj+nuX32+b6enpEBEMHz4cGo0GmzZtwqJFi6BUKgHcPXMEAHPmzMHKlSsxfvx4rFmzBrNmzbI7Abyz/W/cuBG+vr6W18iR9u9sTERERP2DQ+HI398fSqXS5ixLVVWVzZmfVoGBgXbr3d3dMWTIkA5r2m5z9OjRyMvLQ0NDA8rKynDs2DEYjUaEhIRYenN3d0dERITVdsaMGWOZLB4YGIjm5mbU1NR0uv+1a9fCYDBYXmVlZXbriIiIqH9wKByp1Wro9Xrk5ORYLc/JyUFsbKzd98TExNjUZ2dnIzo6GiqVqsMae9v08vJCUFAQampqcODAAcyZM8fS2xNPPIELFy5Y1V+8eBE6nQ4AoNfroVKprPZVUVGBs2fPttu/RqOBj4+P1YuIiIj6MXHQzp07RaVSSVpamhQXF0tKSop4eXnJ1atXRURkzZo1kpSUZKm/cuWKeHp6ysqVK6W4uFjS0tJEpVLJxx9/bKn54osvRKlUSmpqqpw/f15SU1PF3d1djh49aqnZv3+/7Nu3T65cuSLZ2dkSGRkpEydOlObmZkvNJ598IiqVSt577z25dOmS/Nd//ZcolUo5dOiQpSY5OVlGjBghBw8elJMnT8ozzzwjkZGRYjKZOnX8BoNBAIjBYHB06IiIiMhJHPn8djgciYhs3rxZdDqdqNVqiYqKkry8PMu6xYsXy9SpU63qc3NzZcKECaJWq2XUqFGydetWm23u2rVLwsLCRKVSSXh4uGRmZlqtz8jIkNDQUFGr1RIYGCjLli2T2tpam+2kpaXJI488IlqtViIjI+XTTz+1Wt/Y2CjLly8XPz8/8fDwkFmzZklpaWmnj53hiIiIyPU48vnt8H2OBjqDwYCHHnoIZWVlvMRGRETkIurq6jBy5EjU1tbC19e3w1r3Xuqp36ivrwcAfmuNiIjIBdXX1983HPHMkYNaWlpw/fp1eHt7d3j7gq5oTbU8K9XzONa9h2PdezjWvYdj3Xu6a6xFBPX19QgODoabW8ffR+OZIwe5ublhxIgRPboPfiuu93Csew/HuvdwrHsPx7r3dMdY3++MUasu3QSSiIiIqL9iOCIiIiJqg+GoD9FoNPjFL34BjUbj7Fb6PY517+FY9x6Ode/hWPceZ4w1J2QTERERtcEzR0RERERtMBwRERERtcFwRERERNQGwxERERFRGwxHfcSWLVsQEhICrVYLvV6PQ4cOObsll7dx40Y88cQT8Pb2xrBhwzB37lxcuHDBqkZE8G//9m8IDg6Gh4cHpk2bhnPnzjmp4/5j48aNUCgUSElJsSzjWHef8vJy/OhHP8KQIUPg6emJ8ePHo7Cw0LKeY909TCYTXnvtNYSEhMDDwwOhoaH45S9/iZaWFksNx7rr/u///g/f/e53ERwcDIVCgU8//dRqfWfGtqmpCS+++CL8/f3h5eWF2bNn49q1aw/eXA89/JYcsHPnTlGpVPL+++9LcXGxvPTSS+Ll5SVfffWVs1tzaYmJifKHP/xBzp49K6dOnZJnn31WHn74YWloaLDUpKamire3t2RmZsqZM2dkwYIFEhQUJHV1dU7s3LUdO3ZMRo0aJY8//ri89NJLluUc6+7x9ddfi06nk+eff14KCgqkpKREDh48KF9++aWlhmPdPd544w0ZMmSI7N27V0pKSmTXrl0yaNAgefvtty01HOuu+/Of/yzr16+XzMxMASBZWVlW6zsztsnJyTJ8+HDJycmRkydPytNPPy2RkZFiMpkeqDeGoz5g4sSJkpycbLUsPDxc1qxZ46SO+qeqqioBIHl5eSIi0tLSIoGBgZKammqpuXPnjvj6+sq2bduc1aZLq6+vl0cffVRycnJk6tSplnDEse4+q1evlilTprS7nmPdfZ599ln5l3/5F6tl8+bNkx/96EciwrHuTveGo86MbW1trahUKtm5c6elpry8XNzc3GT//v0P1A8vqzlZc3MzCgsLkZCQYLU8ISEBR44ccVJX/ZPBYAAA+Pn5AQBKSkpQWVlpNfYajQZTp07l2HfRsmXL8OyzzyI+Pt5qOce6++zZswfR0dH4wQ9+gGHDhmHChAl4//33Les51t1nypQp+Mtf/oKLFy8CAE6fPo3Dhw9j5syZADjWPakzY1tYWAij0WhVExwcjHHjxj3w+PPBs05WXV0Ns9mMgIAAq+UBAQGorKx0Ulf9j4hg1apVmDJlCsaNGwcAlvG1N/ZfffVVr/fo6nbu3InCwkKcOHHCZh3HuvtcuXIFW7duxapVq7Bu3TocO3YMK1asgEajwY9//GOOdTdavXo1DAYDwsPDoVQqYTab8R//8R/44Q9/CIC/1z2pM2NbWVkJtVqNwYMH29Q86Ocnw1EfoVAorH4WEZtl1HXLly/H3/72Nxw+fNhmHcf+wZWVleGll15CdnY2tFptu3Uc6wfX0tKC6Oho/OpXvwIATJgwAefOncPWrVvx4x//2FLHsX5wGRkZ2L59O/70pz9h7NixOHXqFFJSUhAcHIzFixdb6jjWPacrY9sd48/Lak7m7+8PpVJpk3KrqqpsEjN1zYsvvog9e/bg888/x4gRIyzLAwMDAYBj3w0KCwtRVVUFvV4Pd3d3uLu7Iy8vD5s2bYK7u7tlPDnWDy4oKAgRERFWy8aMGYPS0lIA/L3uTq+88grWrFmDhQsX4jvf+Q6SkpKwcuVKbNy4EQDHuid1ZmwDAwPR3NyMmpqadmu6iuHIydRqNfR6PXJycqyW5+TkIDY21kld9Q8iguXLl+OTTz7BX//6V4SEhFitDwkJQWBgoNXYNzc3Iy8vj2PvoH/6p3/CmTNncOrUKcsrOjoazz33HE6dOoXQ0FCOdTd58sknbW5JcfHiReh0OgD8ve5Ot2/fhpub9cekUqm0fJWfY91zOjO2er0eKpXKqqaiogJnz5598PF/oOnc1C1av8qflpYmxcXFkpKSIl5eXnL16lVnt+bS/vVf/1V8fX0lNzdXKioqLK/bt29balJTU8XX11c++eQTOXPmjPzwhz/k13C7Sdtvq4lwrLvLsWPHxN3dXf7jP/5DLl26JB999JF4enrK9u3bLTUc6+6xePFiGT58uOWr/J988on4+/vLq6++aqnhWHddfX29FBUVSVFRkQCQ3/72t1JUVGS5jU1nxjY5OVlGjBghBw8elJMnT8ozzzzDr/L3J5s3bxadTidqtVqioqIsXzenrgNg9/WHP/zBUtPS0iK/+MUvJDAwUDQajTz11FNy5swZ5zXdj9wbjjjW3ed///d/Zdy4caLRaCQ8PFzee+89q/Uc6+5RV1cnL730kjz88MOi1WolNDRU1q9fL01NTZYajnXXff7553b/H7148WIR6dzYNjY2yvLly8XPz088PDxk1qxZUlpa+sC9KUREHuzcExEREVH/wTlHRERERG0wHBERERG1wXBERERE1AbDEREREVEbDEdEREREbTAcEREREbXBcERERETUBsMRERERURsMR0RERERtMBwRERERtcFwRERERNQGwxERERFRG/8/5s43px7svSYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize your cost function over each iteration.\n",
    "plt.plot(np.arange(num_iterations), cost_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.5409836065573771\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.54      1.00      0.70        33\n",
      "           1       0.00      0.00      0.00        28\n",
      "\n",
      "    accuracy                           0.54        61\n",
      "   macro avg       0.27      0.50      0.35        61\n",
      "weighted avg       0.29      0.54      0.38        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute and print metrics for you SVM on the testing dataset.\n",
    "from sklearn.metrics import accuracy_score\n",
    "pred_y = predict_svm(X_test, weights)\n",
    "y_test[y_test == 'Yes'] = 1\n",
    "y_test[y_test == 'No'] = -1\n",
    "y_test = y_test.astype(np.int32)\n",
    "#print(type(pred_y[0]))\n",
    "#print(type(y_test[0]))\n",
    "\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, pred_y))\n",
    "\n",
    "print(classification_report(y_test,pred_y, zero_division=0))\n",
    "\n",
    "\n",
    "\n",
    "#y_pred = svm_model.predict(X_test)\n",
    "#y_pred[y_pred == \"Yes\"] = 1\n",
    "#y_pred[y_pred == \"No\"] = -1\n",
    "#y_pred = y_pred.astype(np.int32)\n",
    "#print(y_pred)\n",
    "#print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Q4\n",
    "\n",
    "**Compare SVM results to logistic regression**: Implement a logistic regression to solve this task, using libraries. Compare the performance of the logistic regression model to your SVM models on the testing dataset. You can use metrics like accuracy, precision, recall, the F1-score, and any other metric you think may be useful for this comparison. Which one performs better?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the classification reports, the logistic regression has better percentages for precision, recall, f1-score and accuracy. For the target values of \"No\" (aka -1) is the same, but that is the only identical value for these reports. Across the board, the logistic regression is a better predictor for this data set. SVM could be better with added kernels to help fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement logistic regression.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression( fit_intercept=True)\n",
    "y_train = y_train.astype(np.int64)\n",
    "\n",
    "\n",
    "# Fit the model to your training data.\n",
    "#print(type(X_train))\n",
    "#print(type(y_train))\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6885245901639344\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.73      0.67      0.70        33\n",
      "           1       0.65      0.71      0.68        28\n",
      "\n",
      "    accuracy                           0.69        61\n",
      "   macro avg       0.69      0.69      0.69        61\n",
      "weighted avg       0.69      0.69      0.69        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze the performance of the logistic regression on the testing dataset.\n",
    "y_predi = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test,y_predi)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "report = classification_report(y_test,y_predi)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Q5\n",
    "\n",
    "**Apply a kernel function to your SVM**: Using the Scikit-learn library, revisit the SVM you implemented in Question 1 and experiment with different kernel functions. Can you improve its performance? Describe your results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same data as Q1, by using the default rbf kernel with the gamma parameter set to \"auto\" from the default \"scale\" it significantly imporves all percentages in the classification report. By using a larger coefficient for the kernel, it increased the percentages to be more accurate. Coincidentally, using the kernel=\"linear\" parameter, it gave the same exact results as the logistic regression, implying that they function the same on this particular data set. This shows that these algorithms need to be parameterized to better fit the data rather than using default values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(gamma='auto')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an SVM using SciKit-Learn, and train it on your training set. Experiment with different kernel types.\n",
    "svm_model2 = svm.SVC(kernel='rbf', gamma=\"auto\")\n",
    "svm_model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.62      0.85      0.72        33\n",
      "           1       0.69      0.39      0.50        28\n",
      "\n",
      "    accuracy                           0.64        61\n",
      "   macro avg       0.65      0.62      0.61        61\n",
      "weighted avg       0.65      0.64      0.62        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze the performance of the SVM+kernel on the testing dataset.\n",
    "y_predi2 = svm_model2.predict(X_test)\n",
    "print(classification_report(y_test,y_predi2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "otter": {
   "OK_FORMAT": true,
   "assignment_name": "329e_HW07",
   "tests": {
    "q1": {
     "name": "q1",
     "points": 4,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3_computations": {
     "name": "q3_computations",
     "points": 2,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
