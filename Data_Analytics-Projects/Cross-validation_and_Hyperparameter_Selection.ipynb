{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation and hyperparameter selection\n",
    "For this project, we are going to explore the cross-validation testing methodology. We'll use it to get error estimates on two algorithms we've presented in class:\n",
    "  - Linear Regression\n",
    "  - Decision Tree classification\n",
    "\n",
    "You'll also get to implement a much more precise method than gradient descent to compute the optimal coefficients for linear regression, using the **least-squares** method.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the libraries you will use for this project.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "**Calculate Generalized Error on Linear Regression with $k$-fold Cross Validation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.1\n",
    "**Load in the scikit-learn `diabetes` data set.**\n",
    "\n",
    "Documentation is [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html). \n",
    "- Name your feature DataFrame (the independent variables) `df_X`.\n",
    "- Name your target series (the dependent variable) `s_y`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10) (442,)\n"
     ]
    }
   ],
   "source": [
    "df_X, s_y = load_diabetes(return_X_y=True,as_frame=True)\n",
    "# The sizes should be (442, 10) and (442,).\n",
    "s_y = s_y.to_numpy()\n",
    "#print(type(df_X), type(s_y))\n",
    "print(df_X.shape, s_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Q1.2 \n",
    "\n",
    "**Define a function that creates a linear least-squares regression model**\n",
    "\n",
    "- Your function should take in two parameters:\n",
    "    - `A` is the feature DataFrame (i.e. the `df_X` you got in Q1.1.)\n",
    "    - `b` is the target Series (i.e. the `s_y` you got in Q1.1.)\n",
    "- Your function should return a least-squares solution $x$ to the linear matrix equation $Ax = b$.\n",
    "    - $A$ contains the features for each sample. (Based on `df_X` in Q1.1)\n",
    "    - $x$ (the solution) is a vector of coefficients for each feature (column) in $A$. (for you to find)\n",
    "    - $b$ (the target) contains the target values of each sample. (Based on `s_y` in Q1.1)\n",
    "    - You should also fit an intercept to your function. You can do this by augmenting `df_X` ($A$) with an additional column of `1`s. Name this column `intercept` (all lower case).\n",
    "- Once you get $x$ from your function, you will use it in later parts as the coefficients in $x$ to form a linear regression model to estimate the target value for any set of feature values.\n",
    "\n",
    "Notes:\n",
    "-  You can not use any libraries outside of pandas and numpy. \n",
    "- *Hint*: The following function may be useful: [numpy.linalg.lstsq](https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_intercept(df_X: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Copy a DataFrame. Then, add an intercept column of just '1's to\n",
    "    the copy. Return the improved DataFrame.\"\"\"\n",
    "    df_X_copy = df_X\n",
    "    df_X_copy[\"intercept\"] = 1\n",
    "    return df_X_copy\n",
    "\n",
    "def linear_regression(A: pd.DataFrame, b: pd.Series) -> np.ndarray:\n",
    "    \"\"\"Generate a least-squares linear regression model given a DataFrame of\n",
    "    features and a Series of target values. \n",
    "    \n",
    "    Do not add an intercept to A inside this function. It should be done\n",
    "    before passing it in.\n",
    "\n",
    "    You should return x, the coefficients of the linear regression model.\n",
    "    \"\"\"\n",
    "    x = np.linalg.lstsq(A,b,rcond=None)\n",
    "    return x[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -10.01219782 -239.81908937  519.83978679  324.39042769 -792.18416163\n",
      "  476.74583782  101.04457032  177.06417623  751.27932109   67.62538639\n",
      "  152.13348416] (11,)\n"
     ]
    }
   ],
   "source": [
    "# Add an intercept to df_X\n",
    "df_X_intercept = add_intercept(df_X)\n",
    "#print(df_X_intercept)\n",
    "\n",
    "# Compute your least squares solution\n",
    "x = linear_regression(df_X_intercept, s_y)\n",
    "#x = x[0]\n",
    "\n",
    "# The shape of x should be (11,).\n",
    "print(x, x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Q1.3 \n",
    "\n",
    "**Define a function that partitions the dataframe and series data into dictionaries**\n",
    "\n",
    "- Your function should take in three parameters:\n",
    "    - `df_X`, as before\n",
    "    - `s_y`, as before\n",
    "    - `k`, the number of partitions\n",
    "- Your function should return two dictionaries:\n",
    "    - `dict_k_X`: Partitions of the feature DataFrame\n",
    "    - `dict_s_y`: Partitions of the target Series\n",
    "    - For each dictionary:\n",
    "        - *Keys* are the value `k`, i.e. the partition number (1 to `k` inclusive)\n",
    "        - *Values* are the features DataFrame/target Series of the samples *inside that partition* (make sure the features match the targets match.)\n",
    "\n",
    "`dict_k_X` will have the dataframe of the training data that contains approximately $\\frac{1}{k}$ of the data (variation due to randomness is acceptable). Likewise, `dict_s_y` will have the series of the target data that contains approximately $\\frac{1}{k}$ of the data. Please note the **features and targets must match the same rows in the DataFrame**.\n",
    "\n",
    "Finally, call that function with $k=5$ and create the dictionaries `dict_k_X` and `dict_k_y`. \n",
    "- Make sure you use `df_X_intercept` from Q1.2 so you can apply that to your least-squares model.\n",
    "- Print out the number of rows in each fold.  \n",
    "- Check that the number of data points in each partition totals the number of data points in the entire dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def partition_data(df_X: pd.DataFrame, s_y: pd.Series, k: int) -> (dict, dict):\n",
    "    \"\"\"Partition the dataset into k folds (partitions). \n",
    "    \n",
    "    You should return two dictionaries, which contain the k partitions \n",
    "    over the feature DataFrame and target value Series.\n",
    "    \"\"\"\n",
    "    df_l = int(len(df_X)/k) #88\n",
    "    df_m = len(df_X)%k # 2\n",
    "    \n",
    "    sy_l = int(len(s_y)/k) #88\n",
    "    sy_m = len(s_y)%k #2\n",
    "    \n",
    "    dict_k_X = {}\n",
    "    dict_k_y = {}\n",
    "    \n",
    "    for i in range(1,k+1):\n",
    "        if(i == k):\n",
    "            dict_k_X[i] = df_X.iloc[(i-1)*df_l: (i *df_l) + df_m]\n",
    "            dict_k_y[i] = s_y[(i-1)*sy_l: (i * sy_l) + sy_m]\n",
    "            #print((i-1)*df_l)\n",
    "            #print((i *df_l) + df_m)\n",
    "            \n",
    "        else:\n",
    "            dict_k_X[i] = df_X.iloc[(i-1)*df_l: i *df_l]\n",
    "            dict_k_y[i] = s_y[(i-1)*sy_l: i * sy_l]\n",
    "            #print((i-1) *df_l)\n",
    "            #print(i *df_l)\n",
    "    \n",
    "\n",
    "    # Construct the dictionaries.\n",
    "    return (dict_k_X, dict_k_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_X_intercept.iloc[352:442]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold k: 1, features: 88, targets: 88\n",
      "fold k: 2, features: 88, targets: 88\n",
      "fold k: 3, features: 88, targets: 88\n",
      "fold k: 4, features: 88, targets: 88\n",
      "fold k: 5, features: 90, targets: 90\n"
     ]
    }
   ],
   "source": [
    "# Generate the k folds.\n",
    "dict_k_X, dict_k_y = partition_data(df_X_intercept, s_y, 5)\n",
    "#print(dict_k_X)\n",
    "\n",
    "# Check the number of rows in each fold.\n",
    "for k in dict_k_X.keys():\n",
    "    print(f'fold k: {k}, features: {len(dict_k_X[k])}, targets: {len(dict_k_y[k])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Q1.4 \n",
    "\n",
    "**Define a function that calculates a regression metric / error function**\n",
    "\n",
    "- Your function should take two Numpy arrays (You may need to cast your input by calling `.to_numpy()` first):\n",
    "    - `s_y`: The target values \n",
    "    - `s_y_hat`: A series of predictions on the samples for `s_y`.\n",
    "    - The lengths should be equal.\n",
    "- You should calcuate and return the **mean absolute error (MAE)** between `s_y` and `s_y_hat`:\n",
    "    - $MAE = \\sum\\limits_{i=1}^n\\frac{|{s\\_y_i - {s\\_y\\_hat}_i}|}{n}$ \n",
    "\n",
    "Test your function by using the vectors:\n",
    "```\n",
    "x = np.array([1,2,3])\n",
    "y = np.array([2,2,3])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mae(s_y: np.ndarray, s_y_hat: np.ndarray) -> float:\n",
    "    \"\"\"Compute the mean absolute error between the true values s_y\n",
    "    and the predicted values s_y_hat.\"\"\"\n",
    "    s_y, s_y_hat = np.array(s_y), np.array(s_y_hat)\n",
    "    mae = np.mean(np.abs(s_y - s_y_hat))\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Test your function. You should see approximately 0.33.\n",
    "y_true_sample, y_pred_sample = np.array([1,2,3]), np.array([2,2,3])\n",
    "mae_sample = get_mae(y_true_sample, y_pred_sample)\n",
    "print('MAE:', mae_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Q1.5 \n",
    "\n",
    "**Calculate the $MAE$ for each fold**\n",
    "\n",
    "For each of your $k = 5$ folds:\n",
    "1. Split your dataset into a training and testing set as follows:\n",
    "    - The current fold is withheld as the testing set.\n",
    "    - The remaining four folds are used for the training set.\n",
    "    - (Use the partition number key as the test set, and all other partitions as the train set.)\n",
    "2. Train a linear regression model on this fold's training set, using Q 1.2\n",
    "3. Test your model on the testing fold, by multiplying $Ax$ on the coefficients you found. ($Ax = \\hat{b}$, which we will compare to the true targets $b$)\n",
    "4. Compute the MAE of your test predictions vs. the test targets.\n",
    "\n",
    "Store the five MAEs inside the `mae` array, and print the min, max, and mean $MAE$ of your 5 folds. \n",
    "\n",
    "Make sure your `dict_k_X` folds each contain an `intercept` column (this should have already been done for you in Q1.3).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold k: 1\n",
      "X_train, y_train: (354, 11) (354, 1)\n",
      "X_test, y_test  : (88, 11) (88,)\n",
      "x_k             : (11,)\n",
      "y_test_hat      : ()\n",
      "mae             : 179.1567252665175\n",
      "\n",
      "Fold k: 2\n",
      "X_train, y_train: (354, 11) (354, 1)\n",
      "X_test, y_test  : (88, 11) (88,)\n",
      "x_k             : (11,)\n",
      "y_test_hat      : ()\n",
      "mae             : 151.2852885045639\n",
      "\n",
      "Fold k: 3\n",
      "X_train, y_train: (354, 11) (354, 1)\n",
      "X_test, y_test  : (88, 11) (88,)\n",
      "x_k             : (11,)\n",
      "y_test_hat      : ()\n",
      "mae             : 214.36923172734748\n",
      "\n",
      "Fold k: 4\n",
      "X_train, y_train: (354, 11) (354, 1)\n",
      "X_test, y_test  : (88, 11) (88,)\n",
      "x_k             : (11,)\n",
      "y_test_hat      : ()\n",
      "mae             : 238.06197653073949\n",
      "\n",
      "Fold k: 5\n",
      "X_train, y_train: (352, 11) (352, 1)\n",
      "X_test, y_test  : (90, 11) (90,)\n",
      "x_k             : (11,)\n",
      "y_test_hat      : ()\n",
      "mae             : 108.51438407821526\n",
      "\n",
      "MAEs:  [179.1567252665175, 151.2852885045639, 214.36923172734748, 238.06197653073949, 108.51438407821526]\n",
      "min:  108.51438407821526\n",
      "max:  238.06197653073949\n",
      "mean:  178.27752122147672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk_42\\AppData\\Local\\Temp\\ipykernel_13088\\949510406.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_X_train = df_X_train.append(dict_k_X[j])\n",
      "C:\\Users\\tk_42\\AppData\\Local\\Temp\\ipykernel_13088\\949510406.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  s_y_train = s_y_train.append(ss)\n",
      "C:\\Users\\tk_42\\AppData\\Local\\Temp\\ipykernel_13088\\949510406.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_X_train = df_X_train.append(dict_k_X[j])\n",
      "C:\\Users\\tk_42\\AppData\\Local\\Temp\\ipykernel_13088\\949510406.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  s_y_train = s_y_train.append(ss)\n",
      "C:\\Users\\tk_42\\AppData\\Local\\Temp\\ipykernel_13088\\949510406.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_X_train = df_X_train.append(dict_k_X[j])\n",
      "C:\\Users\\tk_42\\AppData\\Local\\Temp\\ipykernel_13088\\949510406.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  s_y_train = s_y_train.append(ss)\n",
      "C:\\Users\\tk_42\\AppData\\Local\\Temp\\ipykernel_13088\\949510406.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  s_y_train = s_y_train.append(ss)\n",
      "C:\\Users\\tk_42\\AppData\\Local\\Temp\\ipykernel_13088\\949510406.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_X_train = df_X_train.append(dict_k_X[j])\n",
      "C:\\Users\\tk_42\\AppData\\Local\\Temp\\ipykernel_13088\\949510406.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  s_y_train = s_y_train.append(ss)\n",
      "C:\\Users\\tk_42\\AppData\\Local\\Temp\\ipykernel_13088\\949510406.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_X_train = df_X_train.append(dict_k_X[j])\n",
      "C:\\Users\\tk_42\\AppData\\Local\\Temp\\ipykernel_13088\\949510406.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  s_y_train = s_y_train.append(ss)\n",
      "C:\\Users\\tk_42\\AppData\\Local\\Temp\\ipykernel_13088\\949510406.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_X_train = df_X_train.append(dict_k_X[j])\n",
      "C:\\Users\\tk_42\\AppData\\Local\\Temp\\ipykernel_13088\\949510406.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  s_y_train = s_y_train.append(ss)\n",
      "C:\\Users\\tk_42\\AppData\\Local\\Temp\\ipykernel_13088\\949510406.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_X_train = df_X_train.append(dict_k_X[j])\n",
      "C:\\Users\\tk_42\\AppData\\Local\\Temp\\ipykernel_13088\\949510406.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  s_y_train = s_y_train.append(ss)\n",
      "C:\\Users\\tk_42\\AppData\\Local\\Temp\\ipykernel_13088\\949510406.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_X_train = df_X_train.append(dict_k_X[j])\n",
      "C:\\Users\\tk_42\\AppData\\Local\\Temp\\ipykernel_13088\\949510406.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  s_y_train = s_y_train.append(ss)\n",
      "C:\\Users\\tk_42\\AppData\\Local\\Temp\\ipykernel_13088\\949510406.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_X_train = df_X_train.append(dict_k_X[j])\n",
      "C:\\Users\\tk_42\\AppData\\Local\\Temp\\ipykernel_13088\\949510406.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  s_y_train = s_y_train.append(ss)\n",
      "C:\\Users\\tk_42\\AppData\\Local\\Temp\\ipykernel_13088\\949510406.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_X_train = df_X_train.append(dict_k_X[j])\n",
      "C:\\Users\\tk_42\\AppData\\Local\\Temp\\ipykernel_13088\\949510406.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  s_y_train = s_y_train.append(ss)\n"
     ]
    }
   ],
   "source": [
    "mae = {} # Use this array to track the MAE of your 5 folds.\n",
    "#dict_k_X, dict_k_y\n",
    "\n",
    "# Place your folds inside these dictionaries:\n",
    "dict_k_X_train = {}\n",
    "dict_k_y_train = {}\n",
    "dict_k_X_test = {}\n",
    "dict_k_y_test = {}\n",
    "\n",
    "for k in dict_k_X.keys():\n",
    "    # Construct the testing and training sets:\n",
    "    df_X_train = pd.DataFrame()\n",
    "    s_y_train = pd.DataFrame()\n",
    "    \n",
    "    df_X_test = dict_k_X[k]\n",
    "    s_y_test = dict_k_y[k]\n",
    "    \n",
    "    for j in range(1,6):\n",
    "        if(j != k):\n",
    "            df_X_train = df_X_train.append(dict_k_X[j])\n",
    "            ss = pd.DataFrame(data=dict_k_y[j])\n",
    "            s_y_train = s_y_train.append(ss)\n",
    "    \n",
    "    # Compute the linear regression coefficients x_k on the training set, then\n",
    "    # the predictions s_y_hat on the test set.\n",
    "    \n",
    "    x_k = linear_regression(df_X_train, s_y_train.squeeze())\n",
    "    s_y_hat = get_mae(df_X_test, x_k)\n",
    "    \n",
    "    # Compute the MAE for the fold.\n",
    "    mae_k = get_mae(s_y_test, s_y_hat)\n",
    "\n",
    "    # Add your folds/MAEs to the dictionaries. (Don't edit these five lines.)\n",
    "    mae[k] = mae_k\n",
    "    dict_k_X_train[k] = df_X_train\n",
    "    dict_k_y_train[k] = s_y_train\n",
    "    dict_k_X_test[k] = df_X_test\n",
    "    dict_k_y_test[k] = s_y_test\n",
    "\n",
    "    # You may find the following print statements useful for debugging.\n",
    "    print('Fold k:', k)\n",
    "    print('X_train, y_train:', df_X_train.shape, s_y_train.shape)\n",
    "    print('X_test, y_test  :', df_X_test.shape, s_y_test.shape)\n",
    "    print('x_k             :', x_k.shape)\n",
    "    print('y_test_hat      :', s_y_hat.shape)\n",
    "    print('mae             :', mae_k)\n",
    "    print()\n",
    "\n",
    "# At the end, print out the MAEs and statistics.\n",
    "print('MAEs: ', list(mae.values()))\n",
    "print('min: ', min(mae.values()))\n",
    "print('max: ', max(mae.values()))\n",
    "print('mean: ', sum(mae.values())/len(mae.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Part 2\n",
    "\n",
    "**Find the best hyperparameter to use in a Decision Tree**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.1 \n",
    "\n",
    "**Load in the scikit-learn `iris` data set**\n",
    "Documentation is [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html). \n",
    "- Name your features DataFrame (the independent variables) `df_X`.\n",
    "- Name your target series (the dependent variable) series `s_y`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n"
     ]
    }
   ],
   "source": [
    "df_X, s_y = load_iris(return_X_y=True,as_frame=True)\n",
    "s_y = s_y.to_numpy()\n",
    "\n",
    "# The sizes should be (150, 4) and (150,).\n",
    "print(df_X.shape, s_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Q2.2 \n",
    "\n",
    "**Partition `df_X` and `s_y` into $5$ partitions of roughly equal size**\n",
    "\n",
    "Just like in Q1.3, partition `df_X` and `s_y` into $k = 5$ partitions (folds) of roughly equal size. Use the `partition_data` function you wrote in Q1.3 to achieve this.\n",
    "\n",
    "As a reminder, you should generate these dictionaries for the iris dataset:\n",
    "- `dict_k_X`: Partitions of the feature DataFrame\n",
    "- `dict_s_y`: Partitions of the target Series\n",
    "- For each dictionary:\n",
    "    - *Keys* are the value `k`, i.e. the partition number (1 to `k` inclusive)\n",
    "    - *Values* are the features DataFrame/target Series of the samples *inside that partition* (make sure the features match the targets match.)\n",
    "\n",
    "Do not add an intercept, as we're working with a decision tree model this time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold k: 1, features: 30, targets: 30\n",
      "fold k: 2, features: 30, targets: 30\n",
      "fold k: 3, features: 30, targets: 30\n",
      "fold k: 4, features: 30, targets: 30\n",
      "fold k: 5, features: 30, targets: 30\n"
     ]
    }
   ],
   "source": [
    "dict_k_X, dict_k_y = partition_data(df_X, s_y, 5)\n",
    "#print(df_X)\n",
    "#print(s_y)\n",
    "# Check the number of rows in each fold.\n",
    "for k in dict_k_X.keys():\n",
    "    print(f'fold k: {k}, features: {len(dict_k_X[k])}, targets: {len(dict_k_y[k])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Q2.3 \n",
    "\n",
    "**Define a function that calculates accuracy**\n",
    "The iris dataset has a categorical target, so we can compute accuracy by checking if each prediction and target are equal. The accuracy is the number of equal elements divided by the total number of elements.\n",
    "\n",
    "- Your function should take two Numpy arrays (You may need to cast your input by calling `.to_numpy()` first):\n",
    "    - `s_y`: The target values \n",
    "    - `s_y_hat`: A series of predictions on the samples for `s_y`.\n",
    "    - The lengths should be equal.\n",
    "- You should calcuate and return the **accuracy** of `s_y_hat` against `s_y`.\n",
    "\n",
    "Test your accuracy function by calling it with the `s_y` loaded from the iris data set and an array of the same length containing all $1$ values. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_accuracy(s_y: np.ndarray, s_y_hat: np.ndarray) -> float:\n",
    "    \"\"\"Compute the accuracy of the predicted values s_y_hat, against \n",
    "    the true values s_y.\"\"\"\n",
    "    count = 0\n",
    "    for i in range(len(s_y)):\n",
    "        if(s_y[i] == s_y_hat[i]):\n",
    "            count += 1\n",
    "    \n",
    "        \n",
    "    acc = count/len(s_y)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function. You should see approximately 0.33.\n",
    "get_accuracy(s_y, np.ones(len(s_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Q2.4 \n",
    "\n",
    "**Using nested cross-validation, find the best hyperparameter for a decision tree**\n",
    "\n",
    "- Create an outer loop for 5-fold cross validation. For each outer fold:\n",
    "\t- Our goal is to pick the best `min_impurity_decrease` for this fold, and train a tree on it.\n",
    "\t- We will use a few choices for `min_impurity_decrease`: $0.1, 0.25, 0.3, 0.4$.\n",
    "\t- For each choice of `min_impurity_decrease`:\n",
    "\t\t- Create an inner loop with 4-fold cross validation. For each inner fold:\n",
    "\t\t\t- Use the scikit-learn [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/tree.html#classification) class to build a decision tree with the current choice of `min_impurity_decrease` we are considering.\n",
    "\t\t\t- Use the Gini Index as your impurity measure / criterion.\n",
    "\t\t\t- *Hint*: We use 4-fold cross validation in the inner loop, so you don't have to re-partition the training set again.\n",
    "\t\t\t- *Hint*: Your 4-folds should not change between hyperparameter choices.\n",
    "\t\t- Calculate the mean accuracy across the 4 inner folds for the candidate `min_impurity_decrease` values.\n",
    "\t- Pick the most accurate `min_impurity_decrease` from the inner loops. If there's a tie, you can pick any of them.\n",
    "\t- Build another DecisionTreeClassifier, and train/test it on the outer fold.\n",
    "\n",
    "Print the following:\n",
    "- For each outer fold:\n",
    "\t- The accuracy of the inner decision trees over the four inner folds for each `min_impurity_decrease` choice.\n",
    "\t- The best (most-accurate) `min_impurity_decrease` as determined in the inner loop.\n",
    "\t- The accuracy of the outer decision tree for the best `min_impurity_decrease`.\n",
    "\n",
    "Your code will be a triply nested loop of the form:\n",
    "```\n",
    "- for k_outer in folds:\n",
    "  - for choice in min_impurity_decrease:\n",
    "    - for k_inner in inner folds:\n",
    "```\n",
    "\n",
    "Your output can look something like this (values will not be exact):\n",
    "```\n",
    "Fold 1:\n",
    "Testing 0.10 min impurity decrease\n",
    "\tAverage accuracy over 4 folds is 94.06%\n",
    "Testing 0.25 min impurity decrease\n",
    "\tAverage accuracy over 4 folds is 86.30%\n",
    "Testing 0.30 min impurity decrease\n",
    "\tAverage accuracy over 4 folds is 63.50%\n",
    "Testing 0.40 min impurity decrease\n",
    "\tAverage accuracy over 4 folds is 27.15%\n",
    "\n",
    "Best min impurity decrease is 0.1\n",
    "Accuracy is 94.4%\n",
    "\n",
    "Fold 2:\n",
    "\n",
    "...\n",
    "\n",
    "```\n",
    "\n",
    "You should also track the accuracy of each of the outer folds, to help you in Q2.5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choices of min_impurity_decrease\n",
    "min_impurity_decrease_choices = np.array([0.1, 0.25, 0.3, 0.4])\n",
    "\n",
    "# Outer loop: 5-fold cross validation\n",
    "for k in dict_k_X.keys():\n",
    "    # Construct the testing and training sets\n",
    "    df_X_train = ...\n",
    "    s_y_train = ...\n",
    "\n",
    "    df_X_test = ...\n",
    "    s_y_test = ...\n",
    "\n",
    "    \n",
    "    # Middle loop: Sweep across hyperparamter values\n",
    "    for min_impurity_decrease in min_impurity_decrease_choices:\n",
    "\n",
    "        # Select the 4 folds (i.e. the 4 k-values) we will use for the inner loop.\n",
    "        inner_ks = ...\n",
    "\n",
    "        # Inner loop: 4-fold cross validation\n",
    "        for k_inner in inner_ks:\n",
    "            # Construct the testing and training sets\n",
    "            df_X_train_inner = ...\n",
    "            s_y_train_inner = ...\n",
    "\n",
    "            df_X_test_inner = ...\n",
    "            s_y_test_inner = ...\n",
    "\n",
    "            \n",
    "            # Create and train the decision tree, with the given hyperparameter value\n",
    "            clf = ...\n",
    "            \n",
    "            # Compute the accuracy of this decision tree on the inner testing fold.\n",
    "            acc_inner = ...\n",
    "        \n",
    "\n",
    "    # Compute the most accuracy min impurity decrease over the 4-folds.\n",
    "    best_min_impurity_decrease = ...\n",
    "    \n",
    "    # Create the decision tree using the best min impurity decrease\n",
    "    clf = ...\n",
    "    clf = clf.fit(df_X_train, s_y_train)\n",
    "            \n",
    "    # Compute the accuracy of this decision tree on the outer testing fold.\n",
    "    acc = ...\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Q2.5 \n",
    "\n",
    "**Show the generalized performance of the classifier** by printing the minimum, maximum, and average accuracy over the outer fold test sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "otter": {
   "OK_FORMAT": true,
   "assignment_name": "329e_HW09",
   "tests": {
    "q1.2": {
     "name": "q1.2",
     "points": 3,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.3": {
     "name": "q1.3",
     "points": 3,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.4": {
     "name": "q1.4",
     "points": 2,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.5": {
     "name": "q1.5",
     "points": 3,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.1": {
     "name": "q2.1",
     "points": 1,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.2": {
     "name": "q2.2",
     "points": 1,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.3": {
     "name": "q2.3",
     "points": 1,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
